#!/usr/bin/env python3
"""
Flask APIæœåŠ¡å™¨ - è¿æ¥RAGç³»ç»Ÿä¸å‰ç«¯ï¼Œé›†æˆGCSå­˜å‚¨
"""

import os
import sys
import json
import time
import tempfile
from datetime import datetime
from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('./src')

from embedding_generation import initialize_vertex_ai, get_text_embeddings
from rag_retrieval import retrieve_relevant_chunks
from rag_generation import generate_answer_with_llm
from data_preprocessing import extract_text_from_pdf, extract_text_from_docx, chunk_text
from gcs_storage import GCSFileManager
from cache_manager import CacheManager

# å¯¼å…¥æ··åˆæ£€ç´¢ç³»ç»Ÿ
try:
    from src.hybrid_retrieval import HybridRetrieval, RetrievalStrategy, RetrievalConfig, hybrid_search
except ImportError:
    from hybrid_retrieval import HybridRetrieval, RetrievalStrategy, RetrievalConfig, hybrid_search

import threading
import queue
from enum import Enum

class ProcessingStatus(Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    ERROR = "error"

# å…¨å±€å˜é‡ç”¨äºç®¡ç†å¤„ç†çŠ¶æ€
PROCESSING_QUEUE = queue.Queue()
PROCESSING_STATUS = {}  # file_id -> {"status": ProcessingStatus, "progress": int, "error": str, "chunks": int}

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨é¢„å…ˆè®¡ç®—çš„embedding
CHUNK_EMBEDDINGS = {}  # chunk_id -> embedding vector

# å…¨å±€å˜é‡ç”¨äºè·Ÿè¸ªå¤„ç†çŠ¶æ€
PROCESSING_FILES = set()  # ç”¨äºè·Ÿè¸ªæ­£åœ¨å¤„ç†çš„æ–‡ä»¶ï¼Œé¿å…é‡å¤å¤„ç†

def background_file_processor():
    """åå°æ–‡ä»¶å¤„ç†çº¿ç¨‹"""
    while True:
        try:
            task = PROCESSING_QUEUE.get(timeout=1)
            if task is None:  # åœæ­¢ä¿¡å·
                break
                
            file_id = task['file_id']
            file_content = task['file_content']
            file_ext = task['file_ext']
            filename = task['filename']
            
            print(f"å¼€å§‹åå°å¤„ç†æ–‡ä»¶: {filename} (ID: {file_id})")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            PROCESSING_STATUS[file_id]["status"] = ProcessingStatus.PROCESSING
            PROCESSING_STATUS[file_id]["progress"] = 10
            
            try:
                # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶è¿›è¡Œæ–‡æœ¬æå–
                temp_path = None
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
                        temp_file.write(file_content)
                        temp_path = temp_file.name
                        temp_file.flush()  # ç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
                    
                    # æ›´æ–°è¿›åº¦
                    PROCESSING_STATUS[file_id]["progress"] = 30
                    
                    # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if not os.path.exists(temp_path):
                        raise FileNotFoundError(f"Temporary file not found: {temp_path}")
                    
                    # æå–æ–‡æœ¬
                    if file_ext == '.pdf':
                        text = extract_text_from_pdf(temp_path)
                    elif file_ext in ['.doc', '.docx']:
                        text = extract_text_from_docx(temp_path)
                    elif file_ext == '.txt':
                        with open(temp_path, 'r', encoding='utf-8') as f:
                            text = f.read()
                    else:
                        raise ValueError(f"Unsupported file type: {file_ext}")
                    
                    # æ›´æ–°è¿›åº¦
                    PROCESSING_STATUS[file_id]["progress"] = 60
                    
                    # åˆ†å—
                    chunks = chunk_text(text, chunk_size=500, overlap_size=100)
                    
                    # æ›´æ–°è¿›åº¦
                    PROCESSING_STATUS[file_id]["progress"] = 80
                    
                    # æ›´æ–°å…¨å±€chunkæ˜ å°„
                    global CHUNK_MAP
                    for i, chunk in enumerate(chunks):
                        chunk_id = f"file_{file_id}_chunk_{i}"
                        CHUNK_MAP[chunk_id] = chunk
                    
                    # å°†æ–°chunksæ·»åŠ åˆ°å…¨å±€çš„é¢„å…ˆç”Ÿæˆçš„chunkç´¢å¼•ä¸­
                    # è¿™æ ·æ£€ç´¢æ—¶å°±èƒ½æ‰¾åˆ°æ–°æ–‡ä»¶çš„å†…å®¹
                    global chunk_id_to_text_map
                    for i, chunk in enumerate(chunks):
                        chunk_id = f"file_{file_id}_chunk_{i}"
                        chunk_id_to_text_map[chunk_id] = chunk
                    
                    # é¢„å…ˆè®¡ç®—embeddingå¹¶å­˜å‚¨
                    print(f"æ­£åœ¨ä¸ºæ–‡ä»¶ {filename} é¢„å…ˆè®¡ç®—embedding...")
                    chunk_texts = list(chunks)
                    if chunk_texts:
                        try:
                            embeddings = get_text_embeddings(chunk_texts)
                            global CHUNK_EMBEDDINGS
                            
                            # å‡†å¤‡ä¸Šä¼ åˆ°å‘é‡æœç´¢çš„æ•°æ®
                            chunk_embeddings_data = []
                            
                            for i, embedding in enumerate(embeddings):
                                chunk_id = f"file_{file_id}_chunk_{i}"
                                CHUNK_EMBEDDINGS[chunk_id] = embedding
                                
                                # æ·»åŠ åˆ°å‘é‡æœç´¢æ•°æ®
                                chunk_embeddings_data.append({
                                    "id": chunk_id,
                                    "embedding": embedding
                                })
                            
                            print(f"æˆåŠŸé¢„å…ˆè®¡ç®—äº† {len(embeddings)} ä¸ªembedding")
                            
                            # ä¸Šä¼ åˆ°å‘é‡æœç´¢ç´¢å¼•
                            print(f"æ­£åœ¨ä¸Šä¼ æ–‡ä»¶ {filename} çš„embeddingåˆ°å‘é‡æœç´¢ç´¢å¼•...")
                            upload_success = upload_embeddings_to_vector_search(chunk_embeddings_data)
                            
                            if upload_success:
                                print(f"æˆåŠŸä¸Šä¼ æ–‡ä»¶ {filename} çš„embeddingåˆ°å‘é‡æœç´¢ç´¢å¼•")
                            else:
                                print(f"ä¸Šä¼ æ–‡ä»¶ {filename} çš„embeddingåˆ°å‘é‡æœç´¢ç´¢å¼•å¤±è´¥")
                                
                        except Exception as e:
                            print(f"é¢„å…ˆè®¡ç®—embeddingå¤±è´¥: {e}")
                            # å³ä½¿embeddingå¤±è´¥ï¼Œæ–‡ä»¶å¤„ç†ä¹Ÿåº”è¯¥ç»§ç»­
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if temp_path and os.path.exists(temp_path):
                        os.remove(temp_path)
                    
                    # æ·»åŠ æ–‡æ¡£åˆ°æ··åˆæ£€ç´¢ç³»ç»Ÿ
                    if hybrid_retrieval:
                        try:
                            print(f"æ­£åœ¨å°†æ–‡ä»¶ {filename} æ·»åŠ åˆ°æ··åˆæ£€ç´¢ç³»ç»Ÿ...")
                            full_text = "\n".join(chunks)
                            success = hybrid_retrieval.add_document(file_id, full_text, filename)
                            if success:
                                print(f"âœ… æ–‡ä»¶ {filename} å·²æ·»åŠ åˆ°æ··åˆæ£€ç´¢ç³»ç»Ÿ")
                            else:
                                print(f"âš ï¸ æ–‡ä»¶ {filename} æ·»åŠ åˆ°æ··åˆæ£€ç´¢ç³»ç»Ÿå¤±è´¥")
                        except Exception as e:
                            print(f"âŒ æ··åˆæ£€ç´¢ç³»ç»Ÿæ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
                    
                    # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
                    PROCESSING_STATUS[file_id]["status"] = ProcessingStatus.COMPLETED
                    PROCESSING_STATUS[file_id]["progress"] = 100
                    PROCESSING_STATUS[file_id]["chunks"] = len(chunks)
                    
                    print(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {filename} - {len(chunks)} ä¸ªæ–‡æœ¬å—")
                    
                except Exception as e:
                    print(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {filename} - {e}")
                    PROCESSING_STATUS[file_id]["status"] = ProcessingStatus.ERROR
                    PROCESSING_STATUS[file_id]["error"] = str(e)
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if temp_path and os.path.exists(temp_path):
                        os.remove(temp_path)
                    
            except Exception as e:
                print(f"å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
                PROCESSING_STATUS[file_id]["status"] = ProcessingStatus.ERROR
                PROCESSING_STATUS[file_id]["error"] = str(e)
                
        except queue.Empty:
            continue
        except Exception as e:
            print(f"åå°å¤„ç†å™¨é”™è¯¯: {e}")
            break

# å¯åŠ¨åå°å¤„ç†çº¿ç¨‹
processing_thread = threading.Thread(target=background_file_processor, daemon=True)
processing_thread.start()

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å¯¼å…¥é¡¹ç›®é…ç½®
try:
    from config import setup_google_credentials, PROJECT_CONFIG, SERVER_CONFIG, PATHS
    # è®¾ç½®Googleè®¤è¯
    setup_google_credentials()
    
    # é…ç½®ä»config.pyè·å–
    PROJECT_ID = PROJECT_CONFIG["project_id"]
    LOCATION = PROJECT_CONFIG["location"]
    ENDPOINT_ID = PROJECT_CONFIG["endpoint_id"]
    BUCKET_NAME = PROJECT_CONFIG["bucket_name"]
    VECTOR_BUCKET_NAME = "vertex_ai_rag_demo_vectors"  # å‘é‡å­˜å‚¨æ¡¶åç§°
    
    print("âœ… é¡¹ç›®é…ç½®åŠ è½½æˆåŠŸ")
    
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
    # é…ç½®ï¼ˆå¤‡ç”¨ï¼‰
    PROJECT_ID = "cy-aispeci-demo"
    LOCATION = "us-central1"
    ENDPOINT_ID = "7934957714357092352"
    BUCKET_NAME = "vertex_ai_rag_demo"
    VECTOR_BUCKET_NAME = "vertex_ai_rag_demo_vectors"

# å‘é‡æœç´¢é…ç½®
INDEX_DISPLAY_NAME = "rag-document-index"
ENDPOINT_DISPLAY_NAME = "rag-document-endpoint"

# å…¨å±€å˜é‡å­˜å‚¨æ–‡æ¡£å—æ˜ å°„å’Œæ–‡ä»¶ä¿¡æ¯
CHUNK_MAP = {}
UPLOADED_FILES = []
chunk_id_to_text_map = {}  # ç”¨äºå­˜å‚¨é¢„å…ˆç”Ÿæˆçš„chunkç´¢å¼•

# ç¼“å­˜ç®¡ç†å™¨
cache_manager = None

# å»¶è¿ŸåŠ è½½æ ‡å¿—
DOCUMENTS_LOADED = False
DOCUMENTS_LOADING = False

# åˆå§‹åŒ–GCSæ–‡ä»¶ç®¡ç†å™¨
gcs_manager = None
vector_index = None
vector_endpoint = None

# æ··åˆæ£€ç´¢ç³»ç»Ÿ
hybrid_retrieval = None

def init_gcs():
    """åˆå§‹åŒ–GCSæ–‡ä»¶ç®¡ç†å™¨"""
    global gcs_manager
    try:
        gcs_manager = GCSFileManager(
            project_id=PROJECT_ID,
            bucket_name=BUCKET_NAME,
            service_account_path=None  # ä½¿ç”¨ç¯å¢ƒå˜é‡GOOGLE_APPLICATION_CREDENTIALS
        )
        print("GCS File Manager initialized successfully")
        return True
    except Exception as e:
        print(f"Failed to initialize GCS: {e}")
        return False

# åˆå§‹åŒ–Vertex AI
def init_vertex_ai():
    """åˆå§‹åŒ–Vertex AI"""
    try:
        # è®¤è¯å·²ç»åœ¨é…ç½®é˜¶æ®µè®¾ç½®å¥½äº†
        initialize_vertex_ai(PROJECT_ID, LOCATION)
        print("Vertex AI initialized successfully")
        return True
    except Exception as e:
        print(f"Failed to initialize Vertex AI: {e}")
        return False

def init_hybrid_retrieval():
    """åˆå§‹åŒ–æ··åˆæ£€ç´¢ç³»ç»Ÿ"""
    global hybrid_retrieval
    try:
        print("ğŸ”§ åˆå§‹åŒ–æ··åˆæ£€ç´¢ç³»ç»Ÿ...")
        
        # åˆ›å»ºæ£€ç´¢é…ç½®
        config = RetrievalConfig(
            num_candidates=10,        # æ¯è·¯å¬å›10ä¸ªå€™é€‰
            final_results=5,          # æœ€ç»ˆè¿”å›5ä¸ªç»“æœ
            faiss_weight=0.6,         # FAISSæƒé‡60%
            vertex_weight=0.4,        # Vertex AIæƒé‡40%
            min_similarity=0.3,       # æœ€ä½ç›¸ä¼¼åº¦30%
            enable_reranking=True     # å¯ç”¨é‡æ’åº
        )
        
        # åˆå§‹åŒ–æ··åˆæ£€ç´¢
        hybrid_retrieval = HybridRetrieval(
            config=config,
            project_id=PROJECT_ID,
            location=LOCATION,
            endpoint_id=ENDPOINT_ID
        )
        
        print("âœ… æ··åˆæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ æ··åˆæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        # å³ä½¿æ··åˆæ£€ç´¢å¤±è´¥ï¼Œç³»ç»Ÿä¹Ÿåº”è¯¥èƒ½å¤Ÿè¿è¡Œï¼ˆä½¿ç”¨åŸæœ‰çš„æ£€ç´¢ï¼‰
        return False

def init_vector_search():
    """åˆå§‹åŒ–å‘é‡æœç´¢ç´¢å¼•å’Œç«¯ç‚¹"""
    global vector_index, vector_endpoint
    try:
        print("å¼€å§‹åˆå§‹åŒ–å‘é‡æœç´¢...")
        
        # å°è¯•å¯¼å…¥å‘é‡æœç´¢ç®¡ç†æ¨¡å—
        try:
            from src.vector_search_management import create_or_get_vector_search_index, deploy_index_to_endpoint
            print("æˆåŠŸå¯¼å…¥å‘é‡æœç´¢ç®¡ç†æ¨¡å—")
        except ImportError as e:
            print(f"å¯¼å…¥å‘é‡æœç´¢ç®¡ç†æ¨¡å—å¤±è´¥: {e}")
            return False
        
        print("Initializing Vector Search...")
        
        # åˆ›å»ºæˆ–è·å–å‘é‡æœç´¢ç´¢å¼•
        try:
            print("æ­£åœ¨åˆ›å»ºæˆ–è·å–å‘é‡æœç´¢ç´¢å¼•...")
            vector_index = create_or_get_vector_search_index(
                project_id=PROJECT_ID,
                location=LOCATION,
                index_display_name=INDEX_DISPLAY_NAME,
                description="RAG Document Index for intelligent Q&A",
                dimensions=768  # text-embedding-004 çš„ç»´åº¦
            )
            print(f"å‘é‡æœç´¢ç´¢å¼•è·å–æˆåŠŸ: {vector_index.name}")
        except Exception as e:
            print(f"åˆ›å»ºæˆ–è·å–å‘é‡æœç´¢ç´¢å¼•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        # éƒ¨ç½²ç´¢å¼•åˆ°ç«¯ç‚¹ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
        try:
            print("æ­£åœ¨éƒ¨ç½²ç´¢å¼•åˆ°ç«¯ç‚¹...")
            vector_endpoint = deploy_index_to_endpoint(
                project_id=PROJECT_ID,
                location=LOCATION,
                index=vector_index,
                endpoint_display_name=ENDPOINT_DISPLAY_NAME,
                wait_for_completion=False  # ä¸ç­‰å¾…å®Œæˆï¼Œå…è®¸æœåŠ¡å™¨å¿«é€Ÿå¯åŠ¨
            )
            print(f"å‘é‡æœç´¢ç«¯ç‚¹è·å–æˆåŠŸ: {vector_endpoint.name}")
        except Exception as e:
            print(f"éƒ¨ç½²ç´¢å¼•åˆ°ç«¯ç‚¹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        print(f"Vector Search initialized successfully")
        print(f"Index: {vector_index.name}")
        print(f"Endpoint: {vector_endpoint.name}")
        print("æ³¨æ„ï¼šç´¢å¼•éƒ¨ç½²å¯èƒ½éœ€è¦10-30åˆ†é’Ÿå®Œæˆï¼Œåœ¨æ­¤æœŸé—´å°†ä½¿ç”¨æœ¬åœ°ç›¸ä¼¼åº¦æœç´¢")
        return True
        
    except Exception as e:
        print(f"Failed to initialize Vector Search: {e}")
        import traceback
        traceback.print_exc()
        return False

def upload_embeddings_to_vector_search(chunk_embeddings_data: list):
    """å°†embeddingæ•°æ®ä¸Šä¼ åˆ°å‘é‡æœç´¢ç´¢å¼•"""
    try:
        if not vector_index or not chunk_embeddings_data:
            print("No vector index or no embeddings to upload")
            return False
        
        from src.vector_search_management import upload_embeddings_to_index
        from google.cloud import storage
        import tempfile
        
        # åˆ›å»ºJSONLæ–‡ä»¶
        jsonl_data = []
        for item in chunk_embeddings_data:
            jsonl_data.append({
                "id": item["id"],
                "embedding": item["embedding"]
            })
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as temp_file:
            for item in jsonl_data:
                temp_file.write(json.dumps(item, ensure_ascii=False) + '\n')
            temp_file_path = temp_file.name
        
        # ä¸Šä¼ åˆ°GCS
        storage_client = storage.Client(project=PROJECT_ID)
        try:
            bucket = storage_client.bucket(VECTOR_BUCKET_NAME)
        except Exception:
            # å¦‚æœæ¡¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
            bucket = storage_client.create_bucket(VECTOR_BUCKET_NAME, location=LOCATION)
        
        blob_name = f"embeddings_data/embeddings_{int(time.time())}.jsonl"
        blob = bucket.blob(blob_name)
        blob.upload_from_filename(temp_file_path)
        
        gcs_uri = f"gs://{VECTOR_BUCKET_NAME}/{blob_name}"
        print(f"Embeddings uploaded to GCS: {gcs_uri}")
        
        # ä¸Šä¼ åˆ°å‘é‡æœç´¢ç´¢å¼•
        upload_embeddings_to_index(
            project_id=PROJECT_ID,
            location=LOCATION,
            index_name=vector_index.name,
            gcs_input_uri=gcs_uri
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_file_path)
        
        print(f"Successfully uploaded {len(jsonl_data)} embeddings to Vector Search")
        return True
        
    except Exception as e:
        print(f"Failed to upload embeddings to Vector Search: {e}")
        import traceback
        traceback.print_exc()
        return False

# åŠ è½½ç°æœ‰æ–‡æ¡£å†…å®¹
def load_existing_documents():
    """åŠ è½½ç°æœ‰çš„æ–‡æ¡£å†…å®¹"""
    global CHUNK_MAP, chunk_id_to_text_map
    doc_path = "./docs/æ³•å¾‹çŸ¥è¯†é—®ç­”.docx"
    if os.path.exists(doc_path):
        try:
            print("Loading existing document...")
            text = extract_text_from_docx(doc_path)
            chunks = chunk_text(text, chunk_size=500, overlap_size=100)
            
            chunk_texts = []
            for i, chunk in enumerate(chunks):
                chunk_id = f"chunk_{i}"
                CHUNK_MAP[chunk_id] = chunk
                chunk_id_to_text_map[chunk_id] = chunk
                chunk_texts.append(chunk)
            
            print(f"Loaded {len(chunks)} chunks from existing document")
            
            # å¼‚æ­¥å¤„ç†embeddingï¼Œä¸é˜»å¡æœåŠ¡å™¨å¯åŠ¨
            if chunk_texts:
                try:
                    print(f"æ­£åœ¨ä¸ºç°æœ‰æ–‡æ¡£é¢„å…ˆè®¡ç®—embeddingï¼ˆå¼‚æ­¥å¤„ç†ï¼‰...")
                    # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥å¤„ç†embedding
                    import threading
                    def process_embeddings():
                        try:
                            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨å¤„ç†
                            if "existing_doc" in PROCESSING_FILES:
                                print("ç°æœ‰æ–‡æ¡£embeddingå·²åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡")
                                return
                            
                            PROCESSING_FILES.add("existing_doc")
                            
                            embeddings = get_text_embeddings(chunk_texts)
                            global CHUNK_EMBEDDINGS
                            
                            # å‡†å¤‡ä¸Šä¼ åˆ°å‘é‡æœç´¢çš„æ•°æ®
                            chunk_embeddings_data = []
                            
                            for i, embedding in enumerate(embeddings):
                                chunk_id = f"chunk_{i}"
                                CHUNK_EMBEDDINGS[chunk_id] = embedding
                                
                                # æ·»åŠ åˆ°å‘é‡æœç´¢æ•°æ®
                                chunk_embeddings_data.append({
                                    "id": chunk_id,
                                    "embedding": embedding
                                })
                            
                            print(f"æˆåŠŸé¢„å…ˆè®¡ç®—äº† {len(embeddings)} ä¸ªç°æœ‰æ–‡æ¡£embedding")
                            
                            # æš‚æ—¶è·³è¿‡å‘é‡æœç´¢ä¸Šä¼ 
                            print("è·³è¿‡å‘é‡æœç´¢ä¸Šä¼ ï¼Œä½¿ç”¨æœ¬åœ°embedding")
                                
                        except Exception as e:
                            print(f"å¼‚æ­¥å¤„ç†ç°æœ‰æ–‡æ¡£embeddingå¤±è´¥: {e}")
                        finally:
                            PROCESSING_FILES.discard("existing_doc")
                    
                    # åœ¨åå°çº¿ç¨‹ä¸­å¤„ç†embedding
                    embedding_thread = threading.Thread(target=process_embeddings, daemon=True)
                    embedding_thread.start()
                    print("ç°æœ‰æ–‡æ¡£embeddingå¤„ç†å·²åœ¨åå°å¯åŠ¨")
                    
                except Exception as e:
                    print(f"å¯åŠ¨ç°æœ‰æ–‡æ¡£embeddingå¤„ç†å¤±è´¥: {e}")
            
        except Exception as e:
            print(f"Failed to load existing document: {e}")
    else:
        print("No existing document found")

def lazy_load_documents():
    """å»¶è¿ŸåŠ è½½æ–‡æ¡£æ•°æ®"""
    global DOCUMENTS_LOADED, DOCUMENTS_LOADING, cache_manager
    
    if DOCUMENTS_LOADED or DOCUMENTS_LOADING:
        return
    
    DOCUMENTS_LOADING = True
    
    try:
        print("ğŸ”„ å»¶è¿ŸåŠ è½½æ–‡æ¡£æ•°æ®...")
        
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        cache_manager = CacheManager()
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½ç°æœ‰æ–‡æ¡£
        cached_chunks = cache_manager.get_cached_chunks("existing_doc")
        if cached_chunks:
            print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½äº† {len(cached_chunks)} ä¸ªç°æœ‰æ–‡æ¡£å—")
            
            # åŠ è½½åˆ°å†…å­˜
            for i, chunk in enumerate(cached_chunks):
                chunk_id = f"chunk_{i}"
                CHUNK_MAP[chunk_id] = chunk
                chunk_id_to_text_map[chunk_id] = chunk
            
            # ä»ç¼“å­˜åŠ è½½embeddings
            chunk_ids = [f"chunk_{i}" for i in range(len(cached_chunks))]
            cached_embeddings = cache_manager.get_cached_embeddings(chunk_ids)
            if cached_embeddings:
                CHUNK_EMBEDDINGS.update(cached_embeddings)
                print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½äº† {len(cached_embeddings)} ä¸ªembeddings")
        else:
            # å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œå¼‚æ­¥åŠ è½½ç°æœ‰æ–‡æ¡£
            print("â³ ç°æœ‰æ–‡æ¡£ç¼“å­˜ä¸å­˜åœ¨ï¼Œå¼‚æ­¥åŠ è½½...")
            threading.Thread(target=load_existing_documents_async, daemon=True).start()
        
        # å¼‚æ­¥åŠ è½½GCSæ–‡ä»¶
        print("â³ å¼‚æ­¥åŠ è½½GCSæ–‡ä»¶...")
        threading.Thread(target=load_gcs_files_async, daemon=True).start()
        
        DOCUMENTS_LOADED = True
        print("âœ… å»¶è¿ŸåŠ è½½å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ å»¶è¿ŸåŠ è½½å¤±è´¥: {e}")
    finally:
        DOCUMENTS_LOADING = False

def load_existing_documents_async():
    """å¼‚æ­¥åŠ è½½ç°æœ‰æ–‡æ¡£"""
    try:
        doc_path = "./docs/æ³•å¾‹çŸ¥è¯†é—®ç­”.docx"
        if os.path.exists(doc_path):
            print("ğŸ”„ å¼‚æ­¥åŠ è½½ç°æœ‰æ–‡æ¡£...")
            
            # è®¡ç®—æ–‡ä»¶hash
            file_hash = cache_manager.get_file_hash(doc_path) if cache_manager else None
            
            # æ£€æŸ¥ç¼“å­˜
            cached_chunks = cache_manager.get_cached_chunks("existing_doc", file_hash) if cache_manager else None
            
            if cached_chunks:
                print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½äº† {len(cached_chunks)} ä¸ªç°æœ‰æ–‡æ¡£å—")
                
                # åŠ è½½åˆ°å†…å­˜
                for i, chunk in enumerate(cached_chunks):
                    chunk_id = f"chunk_{i}"
                    CHUNK_MAP[chunk_id] = chunk
                    chunk_id_to_text_map[chunk_id] = chunk
                
                # ä»ç¼“å­˜åŠ è½½embeddings
                chunk_ids = [f"chunk_{i}" for i in range(len(cached_chunks))]
                cached_embeddings = cache_manager.get_cached_embeddings(chunk_ids) if cache_manager else {}
                if cached_embeddings:
                    CHUNK_EMBEDDINGS.update(cached_embeddings)
                    print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½äº† {len(cached_embeddings)} ä¸ªembeddings")
            else:
                # é‡æ–°å¤„ç†æ–‡æ¡£
                print("â³ é‡æ–°å¤„ç†ç°æœ‰æ–‡æ¡£...")
                text = extract_text_from_docx(doc_path)
                chunks = chunk_text(text, chunk_size=500, overlap_size=100)
                
                # æ›´æ–°å†…å­˜
                for i, chunk in enumerate(chunks):
                    chunk_id = f"chunk_{i}"
                    CHUNK_MAP[chunk_id] = chunk
                    chunk_id_to_text_map[chunk_id] = chunk
                
                # ç¼“å­˜æ–‡æ¡£å—
                if cache_manager:
                    cache_manager.cache_chunks("existing_doc", chunks, file_hash)
                
                # å¼‚æ­¥è®¡ç®—embeddings
                print("â³ å¼‚æ­¥è®¡ç®—embeddings...")
                threading.Thread(target=compute_embeddings_async, args=(chunks, "existing_doc"), daemon=True).start()
                
                print(f"âœ… å¼‚æ­¥åŠ è½½äº† {len(chunks)} ä¸ªç°æœ‰æ–‡æ¡£å—")
                
    except Exception as e:
        print(f"âŒ å¼‚æ­¥åŠ è½½ç°æœ‰æ–‡æ¡£å¤±è´¥: {e}")

def load_gcs_files_async():
    """å¼‚æ­¥åŠ è½½GCSæ–‡ä»¶"""
    try:
        if not gcs_manager:
            print("âŒ GCS manager not available")
            return
            
        print("ğŸ”„ å¼‚æ­¥åŠ è½½GCSæ–‡ä»¶...")
        gcs_files = gcs_manager.list_files()
        print(f"ğŸ“ å‘ç° {len(gcs_files)} ä¸ªGCSæ–‡ä»¶")
        
        # æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼Œæ¯æ‰¹æœ€å¤š5ä¸ª
        batch_size = 5
        for i in range(0, len(gcs_files), batch_size):
            batch = gcs_files[i:i + batch_size]
            
            # ä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºçº¿ç¨‹
            batch_threads = []
            for gcs_file in batch:
                thread = threading.Thread(
                    target=process_gcs_file_async,
                    args=(gcs_file,),
                    daemon=True
                )
                batch_threads.append(thread)
                thread.start()
            
            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            for thread in batch_threads:
                thread.join()
                
        print("âœ… GCSæ–‡ä»¶å¼‚æ­¥åŠ è½½å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ å¼‚æ­¥åŠ è½½GCSæ–‡ä»¶å¤±è´¥: {e}")

def process_gcs_file_async(gcs_file):
    """å¼‚æ­¥å¤„ç†å•ä¸ªGCSæ–‡ä»¶"""
    try:
        file_id = gcs_file['file_id']
        file_name = gcs_file['file_name']
        
        print(f"ğŸ”„ å¤„ç†æ–‡ä»¶: {file_name}")
        
        # æ£€æŸ¥ç¼“å­˜
        cached_chunks = cache_manager.get_cached_chunks(file_id) if cache_manager else None
        if cached_chunks:
            print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½æ–‡ä»¶ {file_name}: {len(cached_chunks)} ä¸ªå—")
            
            # åŠ è½½åˆ°å†…å­˜
            for i, chunk in enumerate(cached_chunks):
                chunk_id = f"file_{file_id}_chunk_{i}"
                CHUNK_MAP[chunk_id] = chunk
                chunk_id_to_text_map[chunk_id] = chunk
            
            # ä»ç¼“å­˜åŠ è½½embeddings
            chunk_ids = [f"file_{file_id}_chunk_{i}" for i in range(len(cached_chunks))]
            cached_embeddings = cache_manager.get_cached_embeddings(chunk_ids) if cache_manager else {}
            if cached_embeddings:
                CHUNK_EMBEDDINGS.update(cached_embeddings)
            
            # æ·»åŠ åˆ°æ–‡ä»¶åˆ—è¡¨
            file_info = {
                'id': file_id,
                'name': file_name,
                'size': gcs_file['size'],
                'type': gcs_file['content_type'],
                'uploadedAt': int(datetime.fromisoformat(gcs_file['created'].replace('Z', '+00:00')).timestamp()) if gcs_file['created'] else int(time.time()),
                'chunks': len(cached_chunks),
                'gcs_info': gcs_file
            }
            UPLOADED_FILES.append(file_info)
            return
        
        # å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œé‡æ–°å¤„ç†æ–‡ä»¶
        print(f"â³ é‡æ–°å¤„ç†æ–‡ä»¶: {file_name}")
        
        # ä¸‹è½½æ–‡ä»¶
        temp_path = gcs_manager.save_to_temp_file(file_id, file_name)
        if not temp_path:
            print(f"âŒ æ— æ³•ä¸‹è½½æ–‡ä»¶: {file_name}")
            return
            
        # æå–æ–‡æœ¬
        file_ext = os.path.splitext(file_name)[1].lower()
        if not file_ext:
            content_type = gcs_file.get('content_type', '').lower()
            if 'pdf' in content_type:
                file_ext = '.pdf'
            elif 'wordprocessingml' in content_type or 'msword' in content_type:
                file_ext = '.docx'
            elif 'text/plain' in content_type:
                file_ext = '.txt'
        
        text = None
        if file_ext == '.pdf':
            text = extract_text_from_pdf(temp_path)
        elif file_ext in ['.doc', '.docx']:
            text = extract_text_from_docx(temp_path)
        elif file_ext == '.txt':
            with open(temp_path, 'r', encoding='utf-8') as f:
                text = f.read()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_path):
            os.remove(temp_path)
        
        if text:
            # åˆ†å—
            chunks = chunk_text(text, chunk_size=500, overlap_size=100)
            
            # æ›´æ–°å†…å­˜
            for i, chunk in enumerate(chunks):
                chunk_id = f"file_{file_id}_chunk_{i}"
                CHUNK_MAP[chunk_id] = chunk
                chunk_id_to_text_map[chunk_id] = chunk
            
            # ç¼“å­˜æ–‡æ¡£å—
            if cache_manager:
                cache_manager.cache_chunks(file_id, chunks)
            
            # å¼‚æ­¥è®¡ç®—embeddings
            threading.Thread(target=compute_embeddings_async, args=(chunks, file_id), daemon=True).start()
            
            # æ·»åŠ åˆ°æ–‡ä»¶åˆ—è¡¨
            file_info = {
                'id': file_id,
                'name': file_name,
                'size': gcs_file['size'],
                'type': gcs_file['content_type'],
                'uploadedAt': int(datetime.fromisoformat(gcs_file['created'].replace('Z', '+00:00')).timestamp()) if gcs_file['created'] else int(time.time()),
                'chunks': len(chunks),
                'gcs_info': gcs_file
            }
            UPLOADED_FILES.append(file_info)
            
            print(f"âœ… æˆåŠŸå¤„ç†æ–‡ä»¶ {file_name}: {len(chunks)} ä¸ªå—")
        else:
            print(f"âŒ æ— æ³•æå–æ–‡æœ¬: {file_name}")
            
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {gcs_file.get('file_name', 'unknown')}: {e}")

def compute_embeddings_async(chunks, file_id):
    """å¼‚æ­¥è®¡ç®—embeddings"""
    try:
        if not chunks:
            return
            
        print(f"ğŸ”„ è®¡ç®—embeddings: {file_id}")
        
        embeddings = get_text_embeddings(chunks)
        
        # æ›´æ–°å†…å­˜
        chunk_embeddings = {}
        for i, embedding in enumerate(embeddings):
            if file_id == "existing_doc":
                chunk_id = f"chunk_{i}"
            else:
                chunk_id = f"file_{file_id}_chunk_{i}"
            
            CHUNK_EMBEDDINGS[chunk_id] = embedding
            chunk_embeddings[chunk_id] = embedding
        
        # ç¼“å­˜embeddings
        if cache_manager:
            cache_manager.cache_embeddings(chunk_embeddings)
        
        print(f"âœ… è®¡ç®—å®Œæˆembeddings: {file_id} ({len(embeddings)} ä¸ª)")
        
    except Exception as e:
        print(f"âŒ è®¡ç®—embeddingså¤±è´¥ {file_id}: {e}")

def load_gcs_files():
    """ä»GCSåŠ è½½å·²å­˜åœ¨çš„æ–‡ä»¶"""
    global UPLOADED_FILES, CHUNK_MAP
    try:
        if not gcs_manager:
            print("GCS manager not available")
            return
        
        print("Loading files from GCS...")
        gcs_files = gcs_manager.list_files()
        print(f"Found {len(gcs_files)} files in GCS")
        
        # é™åˆ¶åŒæ—¶å¤„ç†çš„æ–‡ä»¶æ•°é‡ï¼Œé¿å…é˜»å¡
        max_files_to_process = 10
        files_processed = 0
        
        for gcs_file in gcs_files:
            if files_processed >= max_files_to_process:
                print(f"å·²å¤„ç† {max_files_to_process} ä¸ªæ–‡ä»¶ï¼Œå‰©ä½™æ–‡ä»¶å°†åœ¨åå°å¤„ç†")
                break
                
            file_id = gcs_file['file_id']
            file_name = gcs_file['file_name']
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»åœ¨UPLOADED_FILESä¸­
            existing_file = next((f for f in UPLOADED_FILES if f['id'] == file_id), None)
            if existing_file:
                print(f"File {file_name} already loaded, skipping")
                continue
            
            print(f"Loading file: {file_name} (ID: {file_id})")
            
            # å…ˆæ·»åŠ æ–‡ä»¶åˆ°åˆ—è¡¨ï¼Œå³ä½¿å¤„ç†å¤±è´¥ä¹Ÿè¦è®°å½•
            file_info = {
                'id': file_id,
                'name': file_name,
                'size': gcs_file['size'],
                'type': gcs_file['content_type'],
                'uploadedAt': int(datetime.fromisoformat(gcs_file['created'].replace('Z', '+00:00')).timestamp()) if gcs_file['created'] else int(time.time()),
                'chunks': 0,
                'gcs_info': gcs_file
            }
            
            try:
                # ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•ï¼ˆæ·»åŠ è¶…æ—¶ï¼‰
                temp_path = None
                try:
                    temp_path = gcs_manager.save_to_temp_file(file_id, file_name)
                    print(f"File downloaded successfully: {gcs_file['blob_name']}")
                except Exception as e:
                    print(f"Failed to download file {file_name}: {e}")
                    UPLOADED_FILES.append(file_info)
                    continue
                
                # æå–æ–‡æœ¬
                file_ext = os.path.splitext(file_name)[1].lower()
                
                # å¦‚æœæ–‡ä»¶åæ²¡æœ‰æ‰©å±•åï¼Œå°è¯•ä»content_typeæ¨æ–­
                if not file_ext:
                    content_type = gcs_file.get('content_type', '').lower()
                    if 'pdf' in content_type:
                        file_ext = '.pdf'
                    elif 'wordprocessingml' in content_type or 'msword' in content_type:
                        file_ext = '.docx'
                    elif 'text/plain' in content_type:
                        file_ext = '.txt'
                    else:
                        print(f"Cannot determine file type from content_type: {content_type}")
                
                text = None
                
                if file_ext == '.pdf':
                    text = extract_text_from_pdf(temp_path)
                elif file_ext in ['.doc', '.docx']:
                    text = extract_text_from_docx(temp_path)
                elif file_ext == '.txt':
                    with open(temp_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                else:
                    print(f"Unsupported file type: {file_ext} (content_type: {gcs_file.get('content_type', 'unknown')})")
                    # å³ä½¿ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ä¹Ÿè¦æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                    UPLOADED_FILES.append(file_info)
                    continue
                
                if text:
                    # åˆ†å—
                    chunks = chunk_text(text, chunk_size=500, overlap_size=100)
                    
                    # æ›´æ–°chunkæ˜ å°„
                    for i, chunk in enumerate(chunks):
                        chunk_id = f"file_{file_id}_chunk_{i}"
                        CHUNK_MAP[chunk_id] = chunk
                    
                    # å°†æ–°chunksæ·»åŠ åˆ°å…¨å±€çš„é¢„å…ˆç”Ÿæˆçš„chunkç´¢å¼•ä¸­
                    global chunk_id_to_text_map
                    for i, chunk in enumerate(chunks):
                        chunk_id = f"file_{file_id}_chunk_{i}"
                        chunk_id_to_text_map[chunk_id] = chunk
                    
                    # æ›´æ–°æ–‡ä»¶ä¿¡æ¯
                    file_info['chunks'] = len(chunks)
                    print(f"Successfully processed {file_name}: {len(chunks)} chunks")
                    
                    # å¼‚æ­¥å¤„ç†embeddingï¼Œä¸é˜»å¡å¯åŠ¨
                    def process_file_embeddings(file_id, file_name, chunks):
                        try:
                            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨å¤„ç†
                            if file_id in PROCESSING_FILES:
                                print(f"æ–‡ä»¶ {file_name} çš„embeddingå·²åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡")
                                return
                            
                            PROCESSING_FILES.add(file_id)
                            
                            print(f"æ­£åœ¨ä¸ºæ–‡ä»¶ {file_name} é¢„å…ˆè®¡ç®—embedding...")
                            chunk_texts = list(chunks)
                            if chunk_texts:
                                embeddings = get_text_embeddings(chunk_texts)
                                global CHUNK_EMBEDDINGS
                                
                                # å‡†å¤‡ä¸Šä¼ åˆ°å‘é‡æœç´¢çš„æ•°æ®
                                chunk_embeddings_data = []
                                
                                for i, embedding in enumerate(embeddings):
                                    chunk_id = f"file_{file_id}_chunk_{i}"
                                    CHUNK_EMBEDDINGS[chunk_id] = embedding
                                    
                                    # æ·»åŠ åˆ°å‘é‡æœç´¢æ•°æ®
                                    chunk_embeddings_data.append({
                                        "id": chunk_id,
                                        "embedding": embedding
                                    })
                                
                                print(f"æˆåŠŸé¢„å…ˆè®¡ç®—äº† {len(embeddings)} ä¸ªembedding for {file_name}")
                                
                                # æš‚æ—¶è·³è¿‡å‘é‡æœç´¢ä¸Šä¼ 
                                print("è·³è¿‡å‘é‡æœç´¢ä¸Šä¼ ï¼Œä½¿ç”¨æœ¬åœ°embedding")
                        except Exception as e:
                            print(f"å¤„ç†æ–‡ä»¶ {file_name} çš„embeddingå¤±è´¥: {e}")
                        finally:
                            PROCESSING_FILES.discard(file_id)
                    
                    # åœ¨åå°çº¿ç¨‹ä¸­å¤„ç†embedding
                    import threading
                    embedding_thread = threading.Thread(
                        target=process_file_embeddings, 
                        args=(file_id, file_name, chunks),
                        daemon=True
                    )
                    embedding_thread.start()
                else:
                    print(f"Failed to extract text from {file_name}")
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_path and os.path.exists(temp_path):
                    os.remove(temp_path)
                
            except Exception as e:
                print(f"Failed to process GCS file {file_name}: {e}")
                # å³ä½¿å¤„ç†å¤±è´¥ï¼Œä¹Ÿè¦è®°å½•æ–‡ä»¶ä¿¡æ¯
                pass
            
            # æ·»åŠ åˆ°æ–‡ä»¶åˆ—è¡¨
            UPLOADED_FILES.append(file_info)
            files_processed += 1
        
        print(f"Loaded {len(UPLOADED_FILES)} files from GCS")
        print(f"Total chunks available for search: {len(chunk_id_to_text_map)}")
        print(f"Embedding processing is running in background...")
        
    except Exception as e:
        print(f"Failed to load GCS files: {e}")
        import traceback
        traceback.print_exc()

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    hybrid_health = {}
    hybrid_stats = {}
    
    # è·å–æ··åˆæ£€ç´¢ç³»ç»ŸçŠ¶æ€
    if hybrid_retrieval:
        try:
            hybrid_health = hybrid_retrieval.health_check()
            hybrid_stats = hybrid_retrieval.get_stats()
        except Exception as e:
            hybrid_health = {'error': str(e)}
    
    return jsonify({
        'status': 'healthy',
        'timestamp': int(time.time()),
        'vertex_ai_initialized': 'initialize_vertex_ai' in globals(),
        'gcs_initialized': gcs_manager is not None,
        'documents_loaded': DOCUMENTS_LOADED,
        'documents_loading': DOCUMENTS_LOADING,
        'total_files': len(UPLOADED_FILES),
        'total_chunks': len(CHUNK_MAP),
        'cache_stats': cache_manager.get_cache_stats() if cache_manager else None,
        'hybrid_retrieval': {
            'available': hybrid_retrieval is not None,
            'health': hybrid_health,
            'stats': hybrid_stats
        }
    })

@app.route('/chat', methods=['POST'])
def chat():
    """å¤„ç†èŠå¤©æ¶ˆæ¯"""
    try:
        # ç¡®ä¿æ–‡æ¡£å·²åŠ è½½
        if not DOCUMENTS_LOADED:
            print("ğŸ”„ é¦–æ¬¡è°ƒç”¨chatï¼Œè§¦å‘æ–‡æ¡£åŠ è½½...")
            lazy_load_documents()
        
        data = request.get_json()
        message = data.get('message', '').strip()
        
        if not message:
            return jsonify({'error': 'Message is required'}), 400
        
        print(f"\n{'='*60}")
        print(f"ğŸ“ ç”¨æˆ·é—®é¢˜: {message}")
        print(f"{'='*60}")
        
        # ä½¿ç”¨æ··åˆæ£€ç´¢ç³»ç»Ÿ (ä¼˜å…ˆ) æˆ–ä¼ ç»Ÿæ£€ç´¢ (é™çº§)
        print(f"ğŸ” å¼€å§‹æ£€ç´¢ç›¸å…³æ–‡æ¡£å—...")
        print(f"   - é¡¹ç›®ID: {PROJECT_ID}")
        print(f"   - ä½ç½®: {LOCATION}")
        print(f"   - ç«¯ç‚¹ID: {ENDPOINT_ID}")
        print(f"   - å¯ç”¨æ–‡æ¡£å—æ•°é‡: {len(chunk_id_to_text_map)}")
        
        retrieved_chunks = []
        
        # ä¼˜å…ˆä½¿ç”¨æ··åˆæ£€ç´¢ç³»ç»Ÿ
        if hybrid_retrieval:
            try:
                print(f"ğŸš€ ä½¿ç”¨æ··åˆæ£€ç´¢ç³»ç»Ÿ (FAISS + Vertex AI)")
                
                # æ›´æ–°æ··åˆæ£€ç´¢ç³»ç»Ÿçš„æ•°æ®ï¼Œç¡®ä¿ä¸åŸæœ‰ç³»ç»ŸåŒæ­¥
                hybrid_retrieval.chunk_map = chunk_id_to_text_map
                hybrid_retrieval.chunk_embeddings = CHUNK_EMBEDDINGS
                
                # æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦é€‰æ‹©æ£€ç´¢ç­–ç•¥
                if len(message) > 20:
                    strategy = RetrievalStrategy.HYBRID_PARALLEL
                else:
                    strategy = RetrievalStrategy.ADAPTIVE
                
                # æ‰§è¡Œæ··åˆæ£€ç´¢
                hybrid_results = hybrid_search(
                    query=message,
                    hybrid_retrieval=hybrid_retrieval,
                    strategy=strategy
                )
                
                retrieved_chunks = hybrid_results
                print(f"âœ… æ··åˆæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(retrieved_chunks)} ä¸ªç›¸å…³å—")
                
            except Exception as e:
                print(f"âš ï¸ æ··åˆæ£€ç´¢å¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ£€ç´¢: {e}")
                # é™çº§åˆ°ä¼ ç»Ÿæ£€ç´¢
                retrieved_chunks = retrieve_relevant_chunks(
                    project_id=PROJECT_ID,
                    location=LOCATION,
                    endpoint_id=ENDPOINT_ID,
                    query_text=message,
                    num_neighbors=3,
                    chunk_map=chunk_id_to_text_map,
                    chunk_embeddings=CHUNK_EMBEDDINGS
                )
                print(f"âœ… ä¼ ç»Ÿæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(retrieved_chunks)} ä¸ªç›¸å…³å—")
        else:
            # ä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢
            print(f"ğŸ” ä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢ç³»ç»Ÿ (æ··åˆæ£€ç´¢ä¸å¯ç”¨)")
            retrieved_chunks = retrieve_relevant_chunks(
                project_id=PROJECT_ID,
                location=LOCATION,
                endpoint_id=ENDPOINT_ID,
                query_text=message,
                num_neighbors=3,
                chunk_map=chunk_id_to_text_map,
                chunk_embeddings=CHUNK_EMBEDDINGS
            )
            print(f"âœ… ä¼ ç»Ÿæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(retrieved_chunks)} ä¸ªç›¸å…³å—")
        
        # è·å–æ–‡æœ¬å†…å®¹
        relevant_texts = []
        sources = []
        
        print(f"ğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹:")
        print(f"{'-'*60}")
        
        for i, chunk in enumerate(retrieved_chunks):
            chunk_id = chunk.get('id', chunk.get('datapoint_id', ''))
            distance = chunk.get('distance', 0)
            similarity = chunk.get('similarity', 1 - distance)
            
            # è·å–æ–‡æœ¬å†…å®¹ï¼Œä¼˜å…ˆä»è¿”å›ç»“æœä¸­å–ï¼Œå¦åˆ™ä»æ˜ å°„ä¸­å–
            chunk_text = chunk.get('text', '') or chunk_id_to_text_map.get(chunk_id, "")
            
            if chunk_text:
                relevant_texts.append(chunk_text)
                
                # å°è¯•ä»chunk_idä¸­æå–æ–‡ä»¶ä¿¡æ¯
                file_name = "æœªçŸ¥æ–‡æ¡£"
                if chunk_id.startswith("file_"):
                    # æ ¼å¼ï¼šfile_{file_id}_chunk_{index}
                    parts = chunk_id.split('_')
                    if len(parts) >= 4:
                        file_id = '_'.join(parts[1:-2])  # æå–file_idéƒ¨åˆ†
                        # æŸ¥æ‰¾å¯¹åº”çš„æ–‡ä»¶å
                        for uploaded_file in UPLOADED_FILES:
                            if uploaded_file['id'] == file_id:
                                file_name = uploaded_file['name']
                                break
                elif chunk_id.startswith("chunk_"):
                    file_name = "æ³•å¾‹çŸ¥è¯†é—®ç­”.docx"
                
                sources.append({
                    "chunk_id": chunk_id,
                    "file_name": file_name,
                    "similarity": float(similarity),
                    "content_preview": chunk_text[:100] + "..." if len(chunk_text) > 100 else chunk_text
                })
                
                print(f"ğŸ“„ å— {i+1}: {file_name} (å—{chunk_id.split('_')[-1] if '_' in chunk_id else 'N/A'})")
                print(f"   ç›¸ä¼¼åº¦: {similarity:.3f}")
                print(f"   ID: {chunk_id}")
                print(f"   å†…å®¹: {chunk_text[:100]}{'...' if len(chunk_text) > 100 else ''}")
                print(f"   {'-'*40}")
        
        if not relevant_texts:
            print("ğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹:")
            print("------------------------------------------------------------")
            print("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨åŸºç¡€çŸ¥è¯†å›ç­”")
            print("------------------------------------------------------------")
        
        # ç”Ÿæˆå›ç­”
        print(f"\nğŸ¤– å¼€å§‹ç”Ÿæˆå›ç­”...")
        print(f"   è¾“å…¥ä¸Šä¸‹æ–‡é•¿åº¦: {sum(len(text) for text in relevant_texts)} å­—ç¬¦")
        
        start_time = time.time()
        
        # ä½¿ç”¨æ–°çš„ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒç›¸å…³æ€§é˜ˆå€¼å’Œç­”æ¡ˆæ¥æºæ ‡è¯†
        llm_result = generate_answer_with_llm(
            query=message,
            retrieved_chunks=relevant_texts,
            sources=sources,
                            similarity_threshold=0.60  # ç›¸å…³æ€§é˜ˆå€¼ï¼š>60%ä½¿ç”¨RAGæ£€ç´¢ï¼Œ<=60%ä½¿ç”¨åŸºç¡€çŸ¥è¯†
        )
        
        processing_time = time.time() - start_time
        
        # æå–ç»“æœ
        answer = llm_result["answer"]
        answer_source = llm_result["source"]
        confidence = llm_result["confidence"]
        use_rag = llm_result["use_rag"]
        max_similarity = llm_result["max_similarity"]
        
        print(f"âœ… å›ç­”ç”Ÿæˆå®Œæˆ (è€—æ—¶: {processing_time:.2f}ç§’)")
        print(f"\nğŸ’¬ ç”Ÿæˆçš„å›ç­”:")
        print(f"{'-'*60}")
        print(f"{answer}")
        print(f"{'-'*60}")
        
        # æ„å»ºå“åº”ï¼ŒåŒ…å«ç­”æ¡ˆæ¥æºä¿¡æ¯
        response = {
            'answer': answer,
            'sources': sources,
            'processingTime': processing_time,
            'answerSource': answer_source,  # 'rag' æˆ– 'knowledge' æˆ– 'error'
            'confidence': confidence,
            'useRag': use_rag,
            'maxSimilarity': max_similarity,
            'qualityMetrics': {
                'relevanceScore': max_similarity,
                'sourceCount': len(sources),
                'avgSimilarity': sum(s.get('similarity', 0) for s in sources) / len(sources) if sources else 0
            }
        }
        
        print(f"\nğŸ“Š å“åº”ç»Ÿè®¡:")
        print(f"   - å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        print(f"   - æ£€ç´¢å—æ•°: {len(sources)}")
        print(f"   - å›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")
        print(f"   - ç­”æ¡ˆæ¥æº: {answer_source}")
        print(f"   - ç½®ä¿¡åº¦: {confidence:.3f}")
        print(f"   - ä½¿ç”¨RAG: {'æ˜¯' if use_rag else 'å¦'}")
        print(f"   - æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}")
        print(f"{'='*60}\n")
        
        return jsonify(response)
        
    except Exception as e:
        print(f"âŒ èŠå¤©é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/upload', methods=['POST'])
def upload_file():
    """å¤„ç†æ–‡ä»¶ä¸Šä¼  - å¼‚æ­¥æ¨¡å¼"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file provided'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        allowed_extensions = {'.pdf', '.doc', '.docx', '.txt'}
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in allowed_extensions:
            return jsonify({'error': 'Unsupported file type'}), 400
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å° (10MB)
        file.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
        file_size = file.tell()
        file.seek(0)  # é‡ç½®åˆ°æ–‡ä»¶å¼€å¤´
        
        if file_size > 10 * 1024 * 1024:
            return jsonify({'error': 'File size exceeds 10MB limit'}), 400
        
        # ä¿ç•™åŸå§‹æ–‡ä»¶åï¼Œä½†ç¡®ä¿å®‰å…¨æ€§
        original_filename = file.filename
        # åªç§»é™¤è·¯å¾„åˆ†éš”ç¬¦å’Œä¸€äº›å±é™©å­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ç­‰å­—ç¬¦
        filename = original_filename.replace('/', '_').replace('\\', '_').replace('..', '_')
        print(f"æ¥æ”¶æ–‡ä»¶ä¸Šä¼ : {filename} (åŸå§‹: {original_filename})")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        file_content = file.read()
        
        # ä¸Šä¼ åˆ°GCS
        metadata = {
            'original_filename': file.filename,
            'file_extension': file_ext,
            'upload_timestamp': datetime.utcnow().isoformat()
        }
        
        gcs_info = gcs_manager.upload_file(
            file_content=file_content,
            file_name=filename,
            content_type=file.content_type or 'application/octet-stream',
            metadata=metadata
        )
        
        file_id = gcs_info['file_id']
        
        # è®°å½•ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆå¤„ç†å‰ï¼‰
        file_info = {
            'id': file_id,
            'name': filename,
            'size': file_size,
            'type': file.content_type or 'application/octet-stream',
            'uploadedAt': int(time.time()),
            'gcs_info': gcs_info
        }
        
        UPLOADED_FILES.append(file_info)
        
        # åˆå§‹åŒ–å¤„ç†çŠ¶æ€
        PROCESSING_STATUS[file_id] = {
            "status": ProcessingStatus.PENDING,
            "progress": 0,
            "error": None,
            "chunks": 0
        }
        
        # æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—
        PROCESSING_QUEUE.put({
            'file_id': file_id,
            'file_content': file_content,
            'file_ext': file_ext,
            'filename': filename
        })
        
        response = {
            'success': True,
            'fileId': file_id,
            'fileName': filename,
            'message': 'æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åå°å¤„ç†...',
            'gcs_uri': gcs_info['gs_uri'],
            'signed_url': gcs_info['signed_url'],
            'processing_status': 'pending'
        }
        
        print(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå·²åŠ å…¥å¤„ç†é˜Ÿåˆ—: {file_id}")
        return jsonify(response)
        
    except Exception as e:
        print(f"Upload error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/files', methods=['GET'])
def get_files():
    """è·å–æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶"""
    try:
        files_with_status = []
        
        for file_info in UPLOADED_FILES:
            file_id = file_info['id']
            
            # æ£€æŸ¥å¤„ç†çŠ¶æ€
            processing_status = PROCESSING_STATUS.get(file_id)
            
            # æ„å»ºæ–‡ä»¶ä¿¡æ¯
            file_data = {
                'id': file_id,
                'name': file_info['name'],
                'size': file_info['size'],
                'type': file_info['type'],
                'uploadedAt': file_info['uploadedAt'],
                'chunks': file_info.get('chunks', 0),
                'gcs_info': file_info.get('gcs_info', {}),
            }
            
            # æ·»åŠ å¤„ç†çŠ¶æ€ä¿¡æ¯ - ä¼˜å…ˆä½¿ç”¨å®é™…çš„chunksæ•°é‡æ¥åˆ¤æ–­çŠ¶æ€
            chunks_count = file_info.get('chunks', 0)
            
            if processing_status:
                # å¦‚æœæœ‰å¤„ç†çŠ¶æ€è®°å½•ï¼Œä½¿ç”¨å®ƒ
                file_data.update({
                    'status': processing_status['status'].value,
                    'progress': processing_status['progress'],
                    'processed': processing_status['status'] == ProcessingStatus.COMPLETED,
                    'error': processing_status.get('error')
                })
            else:
                # å¦‚æœæ²¡æœ‰å¤„ç†çŠ¶æ€è®°å½•ï¼Œæ ¹æ®chunksæ•°é‡åˆ¤æ–­çœŸå®çŠ¶æ€
                if chunks_count > 0:
                    # æœ‰chunksè¯´æ˜å·²ç»å¤„ç†å®Œæˆ
                    file_data.update({
                        'status': 'completed',
                        'progress': 100,
                        'processed': True
                    })
                else:
                    # æ²¡æœ‰chunksï¼Œæ£€æŸ¥æ˜¯å¦åœ¨GCSä¸­å­˜åœ¨
                    # å¦‚æœå­˜åœ¨ä½†æ²¡æœ‰chunksï¼Œè¯´æ˜æ˜¯pendingçŠ¶æ€
                    file_data.update({
                        'status': 'pending',
                        'progress': 0,
                        'processed': False
                    })
            
            files_with_status.append(file_data)
        
        # æŒ‰ä¸Šä¼ æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        files_with_status.sort(key=lambda x: x['uploadedAt'], reverse=True)
        
        return jsonify({
            'files': files_with_status,
            'total': len(files_with_status)
        })
        
    except Exception as e:
        print(f"Get files error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/upload/<file_id>/status', methods=['GET'])
def get_upload_status(file_id):
    """è·å–æ–‡ä»¶å¤„ç†çŠ¶æ€ - çœŸå®çŠ¶æ€æŸ¥è¯¢"""
    try:
        # æŸ¥æ‰¾æ–‡ä»¶ä¿¡æ¯
        file_info = next((f for f in UPLOADED_FILES if f['id'] == file_id), None)
        if not file_info:
            return jsonify({'error': 'File not found'}), 404
        
        # è·å–å¤„ç†çŠ¶æ€
        status_info = PROCESSING_STATUS.get(file_id)
        if not status_info:
            return jsonify({'error': 'Processing status not found'}), 404
        
        # å¦‚æœå¤„ç†å®Œæˆï¼Œæ›´æ–°æ–‡ä»¶ä¿¡æ¯ä¸­çš„chunksæ•°é‡
        if status_info["status"] == ProcessingStatus.COMPLETED and status_info["chunks"] > 0:
            # æ›´æ–°UPLOADED_FILESä¸­çš„chunksä¿¡æ¯
            for file in UPLOADED_FILES:
                if file['id'] == file_id:
                    file['chunks'] = status_info["chunks"]
                    break
        
        return jsonify({
            'file_id': file_id,
            'status': status_info["status"].value,
            'progress': status_info["progress"],
            'processed': status_info["status"] == ProcessingStatus.COMPLETED,
            'chunks': status_info["chunks"],
            'error': status_info["error"],
            'estimated_time_remaining': calculate_estimated_time(status_info["progress"])
        })
        
    except Exception as e:
        print(f"Status check error: {e}")
        return jsonify({'error': str(e)}), 500

def calculate_estimated_time(progress):
    """è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´ï¼ˆç§’ï¼‰"""
    if progress >= 100:
        return 0
    elif progress >= 80:
        return 5  # æœ€å20%é¢„è®¡5ç§’
    elif progress >= 60:
        return 15  # 60-80%é¢„è®¡15ç§’
    elif progress >= 30:
        return 30  # 30-60%é¢„è®¡30ç§’
    else:
        return 60  # å‰30%é¢„è®¡60ç§’

@app.route('/files/<file_id>/preview', methods=['GET'])
def preview_file(file_id):
    """è·å–æ–‡ä»¶é¢„è§ˆå†…å®¹"""
    try:
        # æŸ¥æ‰¾æ–‡ä»¶ä¿¡æ¯
        file_info = next((f for f in UPLOADED_FILES if f['id'] == file_id), None)
        if not file_info:
            return jsonify({'error': 'File not found'}), 404
        
        # è·å–æ–‡ä»¶çš„æ‰€æœ‰æ–‡æœ¬å—
        file_chunks = []
        for chunk_id, chunk_text in CHUNK_MAP.items():
            if chunk_id.startswith(f"file_{file_id}_chunk_"):
                chunk_index = int(chunk_id.split('_')[-1])
                file_chunks.append({
                    'index': chunk_index,
                    'content': chunk_text,
                    'length': len(chunk_text)
                })
        
        # æŒ‰ç´¢å¼•æ’åº
        file_chunks.sort(key=lambda x: x['index'])
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å—è·å–å®Œæ•´å†…å®¹
        full_text = '\n\n'.join([chunk['content'] for chunk in file_chunks])
        
        # è·å–GCSæ–‡ä»¶ä¿¡æ¯
        gcs_file_info = None
        try:
            gcs_file_info = gcs_manager.get_file_info(file_id, file_info['name'])
        except Exception as e:
            print(f"Failed to get GCS file info: {e}")
        
        # æ„å»ºé¢„è§ˆå“åº”
        preview_data = {
            'fileId': file_id,
            'fileName': file_info['name'],
            'fileSize': file_info['size'],
            'uploadedAt': file_info['uploadedAt'],
            'totalChunks': len(file_chunks),
            'fullText': full_text,
            'chunks': file_chunks,
            'metadata': {
                'wordCount': len(full_text.split()),
                'charCount': len(full_text),
                'type': file_info.get('type', 'unknown')
            },
            'gcs_info': {
                'gs_uri': file_info.get('gcs_info', {}).get('gs_uri'),
                'signed_url': gcs_file_info.get('signed_url') if gcs_file_info else None,
                'public_url': gcs_file_info.get('public_url') if gcs_file_info else None
            }
        }
        
        return jsonify(preview_data)
        
    except Exception as e:
        print(f"Preview error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/files/<file_id>/download', methods=['GET'])
def download_file(file_id):
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        # æŸ¥æ‰¾æ–‡ä»¶ä¿¡æ¯
        file_info = next((f for f in UPLOADED_FILES if f['id'] == file_id), None)
        if not file_info:
            return jsonify({'error': 'File not found'}), 404
        
        # è·å–GCSç­¾åURL
        try:
            signed_url = gcs_manager.get_signed_url(
                file_id=file_id,
                file_name=file_info['name'],
                expiration_hours=1  # 1å°æ—¶æœ‰æ•ˆæœŸ
            )
            
            return jsonify({
                'download_url': signed_url,
                'file_name': file_info['name'],
                'expires_in': 3600  # 1å°æ—¶
            })
            
        except Exception as e:
            print(f"Failed to generate download URL: {e}")
            return jsonify({'error': 'Failed to generate download URL'}), 500
        
    except Exception as e:
        print(f"Download error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/files/<file_id>/delete', methods=['DELETE'])
def delete_file(file_id):
    """åˆ é™¤æ–‡ä»¶"""
    global UPLOADED_FILES, CHUNK_MAP
    try:
        # æŸ¥æ‰¾æ–‡ä»¶ä¿¡æ¯
        file_info = next((f for f in UPLOADED_FILES if f['id'] == file_id), None)
        if not file_info:
            return jsonify({'error': 'File not found'}), 404
        
        # ä»GCSåˆ é™¤æ–‡ä»¶
        success = gcs_manager.delete_file(file_id, file_info['name'])
        
        if success:
            # ä»å†…å­˜ä¸­åˆ é™¤ç›¸å…³æ•°æ®
            
            # åˆ é™¤æ–‡ä»¶è®°å½•
            UPLOADED_FILES = [f for f in UPLOADED_FILES if f['id'] != file_id]
            
            # åˆ é™¤ç›¸å…³çš„æ–‡æœ¬å—
            chunk_keys_to_delete = [key for key in CHUNK_MAP.keys() if key.startswith(f"file_{file_id}_chunk_")]
            for key in chunk_keys_to_delete:
                del CHUNK_MAP[key]
            
            return jsonify({
                'success': True,
                'message': 'File deleted successfully',
                'deleted_chunks': len(chunk_keys_to_delete)
            })
        else:
            return jsonify({'error': 'Failed to delete file from storage'}), 500
        
    except Exception as e:
        print(f"Delete error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/files/<file_id>/chunks', methods=['GET'])
def get_file_chunks(file_id):
    """è·å–æ–‡ä»¶çš„åˆ†å—ä¿¡æ¯"""
    try:
        # æŸ¥æ‰¾æ–‡ä»¶ä¿¡æ¯
        file_info = next((f for f in UPLOADED_FILES if f['id'] == file_id), None)
        if not file_info:
            return jsonify({'error': 'File not found'}), 404
        
        # è·å–åˆ†é¡µå‚æ•°
        page = int(request.args.get('page', 1))
        per_page = int(request.args.get('per_page', 10))
        
        # è·å–æ–‡ä»¶çš„æ‰€æœ‰æ–‡æœ¬å—
        file_chunks = []
        for chunk_id, chunk_text in CHUNK_MAP.items():
            if chunk_id.startswith(f"file_{file_id}_chunk_"):
                chunk_index = int(chunk_id.split('_')[-1])
                file_chunks.append({
                    'id': chunk_id,
                    'index': chunk_index,
                    'content': chunk_text,
                    'length': len(chunk_text),
                    'wordCount': len(chunk_text.split())
                })
        
        # æŒ‰ç´¢å¼•æ’åº
        file_chunks.sort(key=lambda x: x['index'])
        
        # åˆ†é¡µå¤„ç†
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        paginated_chunks = file_chunks[start_idx:end_idx]
        
        return jsonify({
            'fileId': file_id,
            'fileName': file_info['name'],
            'totalChunks': len(file_chunks),
            'page': page,
            'perPage': per_page,
            'totalPages': (len(file_chunks) + per_page - 1) // per_page,
            'chunks': paginated_chunks
        })
        
    except Exception as e:
        print(f"Chunks error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/debug/chunks', methods=['GET'])
def debug_chunks():
    """è°ƒè¯•ç«¯ç‚¹ï¼šæŸ¥çœ‹æ‰€æœ‰chunkä¿¡æ¯"""
    chunk_info = {}
    for chunk_id, chunk_text in CHUNK_MAP.items():
        has_embedding = chunk_id in CHUNK_EMBEDDINGS
        chunk_info[chunk_id] = {
            'preview': chunk_text[:100] + '...' if len(chunk_text) > 100 else chunk_text,
            'length': len(chunk_text),
            'has_embedding': has_embedding,
            'embedding_size': len(CHUNK_EMBEDDINGS[chunk_id]) if has_embedding else 0
        }
    
    return jsonify({
        'total_chunks': len(CHUNK_MAP),
        'total_embeddings': len(CHUNK_EMBEDDINGS),
        'chunks': chunk_info
    })

@app.route('/debug/embedding/<chunk_id>', methods=['GET'])
def debug_embedding(chunk_id):
    """è°ƒè¯•ç«¯ç‚¹ï¼šæ£€æŸ¥ç‰¹å®šchunkçš„embedding"""
    if chunk_id not in CHUNK_MAP:
        return jsonify({'error': 'Chunk not found'}), 404
    
    chunk_text = CHUNK_MAP[chunk_id]
    has_embedding = chunk_id in CHUNK_EMBEDDINGS
    
    # å¦‚æœæ²¡æœ‰embeddingï¼Œå°è¯•é‡æ–°è®¡ç®—
    if not has_embedding:
        print(f"Chunk {chunk_id} æ²¡æœ‰embeddingï¼Œé‡æ–°è®¡ç®—...")
        try:
            from embedding_generation import get_text_embeddings
            embeddings = get_text_embeddings([chunk_text])
            if embeddings and embeddings[0]:
                CHUNK_EMBEDDINGS[chunk_id] = embeddings[0]
                has_embedding = True
                print(f"æˆåŠŸä¸º {chunk_id} è®¡ç®—embedding")
            else:
                print(f"ä¸º {chunk_id} è®¡ç®—embeddingå¤±è´¥")
        except Exception as e:
            print(f"è®¡ç®—embeddingå‡ºé”™: {e}")
    
    return jsonify({
        'chunk_id': chunk_id,
        'chunk_text': chunk_text,
        'text_length': len(chunk_text),
        'has_embedding': has_embedding,
        'embedding_size': len(CHUNK_EMBEDDINGS[chunk_id]) if has_embedding else 0,
        'embedding_preview': CHUNK_EMBEDDINGS[chunk_id][:5] if has_embedding else None
    })

@app.route('/clear/all_embeddings', methods=['POST'])
def clear_all_embeddings():
    """æ¸…ç†æ‰€æœ‰embeddingæ•°æ®ï¼Œé‡æ–°å¼€å§‹"""
    try:
        global CHUNK_MAP, CHUNK_EMBEDDINGS, chunk_id_to_text_map, UPLOADED_FILES, DOCUMENTS_LOADED, cache_manager
        
        # æ¸…ç†å†…å­˜æ•°æ®
        CHUNK_MAP.clear()
        CHUNK_EMBEDDINGS.clear()
        chunk_id_to_text_map.clear()
        UPLOADED_FILES.clear()
        
        # é‡ç½®åŠ è½½çŠ¶æ€
        DOCUMENTS_LOADED = False
        
        # æ¸…ç†ç¼“å­˜
        if cache_manager:
            cache_manager.clear_cache()
        
        return jsonify({
            'success': True,
            'message': 'æ‰€æœ‰embeddingæ•°æ®å·²æ¸…ç†',
            'cleared_data': {
                'chunks': len(CHUNK_MAP),
                'embeddings': len(CHUNK_EMBEDDINGS),
                'files': len(UPLOADED_FILES)
            }
        })
        
    except Exception as e:
        print(f"Clear embeddings error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/rebuild/embeddings', methods=['POST'])
def rebuild_embeddings():
    """é‡æ–°æ„å»ºæ‰€æœ‰embedding"""
    try:
        # å…ˆæ¸…ç†
        global CHUNK_MAP, CHUNK_EMBEDDINGS, chunk_id_to_text_map, UPLOADED_FILES, DOCUMENTS_LOADED
        
        CHUNK_MAP.clear()
        CHUNK_EMBEDDINGS.clear()
        chunk_id_to_text_map.clear()
        UPLOADED_FILES.clear()
        DOCUMENTS_LOADED = False
        
        # å¼ºåˆ¶é‡æ–°åŠ è½½æ–‡æ¡£
        lazy_load_documents()
        
        return jsonify({
            'success': True,
            'message': 'æ­£åœ¨é‡æ–°æ„å»ºembeddingï¼Œè¯·ç¨å€™...',
            'status': 'rebuilding'
        })
        
    except Exception as e:
        print(f"Rebuild embeddings error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/create/clean_chunk', methods=['POST'])
def create_clean_chunk():
    """åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„çº¯å‡€å®šé‡‘chunk"""
    try:
        # åˆ›å»ºä¸€ä¸ªå®Œå…¨çº¯å‡€çš„å®šé‡‘chunk
        clean_text = """å®šé‡‘æ˜¯æŒ‡å½“äº‹äººçº¦å®šç”±ä¸€æ–¹å‘å¯¹æ–¹ç»™ä»˜çš„ï¼Œä½œä¸ºå€ºæƒæ‹…ä¿çš„ä¸€å®šæ•°é¢çš„è´§å¸ï¼Œå®ƒå±äºä¸€ç§æ³•å¾‹ä¸Šçš„æ‹…ä¿æ–¹å¼ï¼Œç›®çš„åœ¨äºä¿ƒä½¿å€ºåŠ¡äººå±¥è¡Œå€ºåŠ¡ï¼Œä¿éšœå€ºæƒäººçš„å€ºæƒå¾—ä»¥å®ç°ã€‚

å®šé‡‘ä¸è®¢é‡‘çš„åŒºåˆ«ï¼š
1. å®šé‡‘å…·æœ‰æ‹…ä¿æ€§è´¨ï¼Œè®¢é‡‘ä¸å…·å¤‡æ‹…ä¿æ€§è´¨
2. å®šé‡‘å—æ³•å¾‹ä¿æŠ¤ï¼Œè®¢é‡‘å¯è§†ä¸ºé¢„ä»˜æ¬¾
3. å®šé‡‘æœ‰åŒå€è¿”è¿˜è§„åˆ™ï¼Œè®¢é‡‘æŒ‰è¿‡é”™æ‰¿æ‹…è´£ä»»
4. å®šé‡‘å¿…é¡»ä»¥ä¹¦é¢å½¢å¼çº¦å®šï¼Œè®¢é‡‘å½¢å¼ç›¸å¯¹çµæ´»

å®šé‡‘çš„æ³•å¾‹æ•ˆåŠ›ï¼š
- ç»™ä»˜å®šé‡‘ä¸€æ–¹ä¸å±¥è¡Œå€ºåŠ¡ï¼Œæ— æƒè¦æ±‚è¿”è¿˜å®šé‡‘
- æ¥å—å®šé‡‘ä¸€æ–¹ä¸å±¥è¡Œå€ºåŠ¡ï¼Œéœ€åŒå€è¿”è¿˜å®šé‡‘
- å€ºåŠ¡å±¥è¡Œåï¼Œå®šé‡‘åº”æŠµä½œä»·æ¬¾æˆ–æ”¶å›"""
        
        # åˆ›å»ºæ–°çš„chunk ID
        new_chunk_id = 'clean_dingjin_chunk'
        
        # æ›´æ–°å…¨å±€æ˜ å°„
        CHUNK_MAP[new_chunk_id] = clean_text
        chunk_id_to_text_map[new_chunk_id] = clean_text
        
        # é‡æ–°è®¡ç®—embedding
        from embedding_generation import get_text_embeddings
        embeddings = get_text_embeddings([clean_text])
        
        if embeddings and embeddings[0]:
            CHUNK_EMBEDDINGS[new_chunk_id] = embeddings[0]
            
            # ç¼“å­˜æ›´æ–°
            if cache_manager:
                cache_manager.cache_embeddings({new_chunk_id: embeddings[0]})
            
            return jsonify({
                'success': True,
                'message': 'åˆ›å»ºäº†å…¨æ–°çš„çº¯å‡€å®šé‡‘chunk',
                'chunk_id': new_chunk_id,
                'text_length': len(clean_text),
                'text_preview': clean_text[:200] + '...'
            })
        else:
            return jsonify({'error': 'Failed to generate embedding for clean text'}), 500
            
    except Exception as e:
        print(f"Create clean chunk error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/fix/chunk_0', methods=['POST'])
def fix_chunk_0():
    """ä¿®å¤chunk_0ï¼šåˆ›å»ºçº¯å‡€çš„å®šé‡‘ç›¸å…³chunk"""
    try:
        # å®šé‡‘ç›¸å…³çš„çº¯å‡€æ–‡æœ¬
        clean_text = """æ³•å¾‹çŸ¥è¯†é—®ç­”
1ã€é—®é¢˜ï¼šåœ¨æ³•å¾‹ä¸­å®šé‡‘ä¸è®¢é‡‘çš„åŒºåˆ«ï¼Ÿ
      ç­”æ¡ˆï¼š"å®šé‡‘"æ˜¯æŒ‡å½“äº‹äººçº¦å®šç”±ä¸€æ–¹å‘å¯¹æ–¹ç»™ä»˜çš„ï¼Œä½œä¸ºå€ºæƒæ‹…ä¿çš„ä¸€å®šæ•°é¢çš„è´§å¸ï¼Œå®ƒå±äºä¸€ç§æ³•å¾‹ä¸Šçš„æ‹…ä¿æ–¹å¼ï¼Œç›®çš„åœ¨äºä¿ƒä½¿å€ºåŠ¡äººå±¥è¡Œå€ºåŠ¡ï¼Œä¿éšœå€ºæƒäººçš„å€ºæƒå¾—ä»¥å®ç°ã€‚ç­¾åˆåŒæ—¶ï¼Œå¯¹å®šé‡‘å¿…éœ€ä»¥ä¹¦é¢å½¢å¼è¿›è¡Œçº¦å®šï¼ŒåŒæ—¶è¿˜åº”çº¦å®šå®šé‡‘çš„æ•°é¢å’Œäº¤ä»˜æœŸé™ã€‚ç»™ä»˜å®šé‡‘ä¸€æ–¹å¦‚æœä¸å±¥è¡Œå€ºåŠ¡ï¼Œæ— æƒè¦æ±‚å¦ä¸€æ–¹è¿”è¿˜å®šé‡‘ï¼›æ¥å—å®šé‡‘çš„ä¸€æ–¹å¦‚æœä¸å±¥è¡Œå€ºåŠ¡ï¼Œéœ€å‘å¦ä¸€æ–¹åŒå€è¿”è¿˜å€ºåŠ¡ã€‚å€ºåŠ¡äººå±¥è¡Œå€ºåŠ¡åï¼Œä¾ç…§çº¦å®šï¼Œå®šé‡‘åº”æŠµä½œä»·æ¬¾æˆ–è€…æ”¶å›ã€‚è€Œ"è®¢é‡‘"ç›®å‰æˆ‘å›½æ³•å¾‹æ²¡æœ‰æ˜ç¡®è§„å®šï¼Œå®ƒä¸å…·å¤‡å®šé‡‘æ‰€å…·æœ‰çš„æ‹…ä¿æ€§è´¨ï¼Œå¯è§†ä¸º"é¢„ä»˜æ¬¾"ï¼Œå½“åˆåŒä¸èƒ½å±¥è¡Œæ—¶ï¼Œé™¤ä¸å¯æŠ—åŠ›å¤–ï¼Œåº”æ ¹æ®åŒæ–¹å½“äº‹äººçš„è¿‡é”™æ‰¿æ‹…è¿çº¦è´£ä»»ã€‚"""
        
        # æ›´æ–°chunk_0çš„æ–‡æœ¬
        CHUNK_MAP['chunk_0'] = clean_text
        
        # é‡æ–°è®¡ç®—embedding
        from embedding_generation import get_text_embeddings
        embeddings = get_text_embeddings([clean_text])
        
        if embeddings and embeddings[0]:
            CHUNK_EMBEDDINGS['chunk_0'] = embeddings[0]
            
            # ç¼“å­˜æ›´æ–°åçš„æ•°æ®
            if cache_manager:
                cache_manager.cache_embeddings({'chunk_0': embeddings[0]})
            
            return jsonify({
                'success': True,
                'message': 'chunk_0å·²ä¿®å¤ä¸ºçº¯å‡€çš„å®šé‡‘ç›¸å…³å†…å®¹',
                'old_length': 500,
                'new_length': len(clean_text),
                'clean_text_preview': clean_text[:200] + '...'
            })
        else:
            return jsonify({'error': 'Failed to generate embedding for clean text'}), 500
            
    except Exception as e:
        print(f"Fix chunk_0 error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/debug/similarity', methods=['POST'])
def debug_similarity():
    """è°ƒè¯•ç«¯ç‚¹ï¼šæµ‹è¯•æŸ¥è¯¢ä¸ç‰¹å®šchunksçš„ç›¸ä¼¼åº¦"""
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        chunk_ids = data.get('chunk_ids', ['chunk_0', 'chunk_3'])
        
        if not query:
            return jsonify({'error': 'Query is required'}), 400
        
        # ç”ŸæˆæŸ¥è¯¢embedding
        from embedding_generation import get_text_embeddings
        import numpy as np
        
        query_embeddings = get_text_embeddings([query])
        if not query_embeddings or not query_embeddings[0]:
            return jsonify({'error': 'Failed to generate query embedding'}), 500
        
        query_vec = np.array(query_embeddings[0])
        
        results = []
        for chunk_id in chunk_ids:
            if chunk_id not in CHUNK_MAP:
                continue
                
            chunk_text = CHUNK_MAP[chunk_id]
            
            # è·å–æˆ–è®¡ç®—chunk embedding
            if chunk_id in CHUNK_EMBEDDINGS:
                chunk_embedding = CHUNK_EMBEDDINGS[chunk_id]
            else:
                chunk_embeddings = get_text_embeddings([chunk_text])
                if chunk_embeddings and chunk_embeddings[0]:
                    chunk_embedding = chunk_embeddings[0]
                    CHUNK_EMBEDDINGS[chunk_id] = chunk_embedding
                else:
                    continue
            
            chunk_vec = np.array(chunk_embedding)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(query_vec, chunk_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(chunk_vec))
            distance = 1 - similarity
            
            results.append({
                'chunk_id': chunk_id,
                'similarity': float(similarity),
                'distance': float(distance),
                'text_preview': chunk_text[:200] + '...' if len(chunk_text) > 200 else chunk_text,
                'embedding_norm': float(np.linalg.norm(chunk_vec)),
                'query_norm': float(np.linalg.norm(query_vec))
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return jsonify({
            'query': query,
            'query_embedding_preview': query_embeddings[0][:5],
            'results': results
        })
        
    except Exception as e:
        print(f"Debug similarity error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

# æ··åˆæ£€ç´¢ç³»ç»Ÿç®¡ç†æ¥å£
@app.route('/hybrid/config', methods=['GET'])
def get_hybrid_config():
    """è·å–æ··åˆæ£€ç´¢ç³»ç»Ÿé…ç½®"""
    if not hybrid_retrieval:
        return jsonify({'error': 'æ··åˆæ£€ç´¢ç³»ç»Ÿæœªåˆå§‹åŒ–'}), 400
    
    try:
        stats = hybrid_retrieval.get_stats()
        return jsonify({
            'success': True,
            'config': stats.get('config', {}),
            'stats': stats
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/hybrid/config', methods=['POST'])
def update_hybrid_config():
    """æ›´æ–°æ··åˆæ£€ç´¢ç³»ç»Ÿé…ç½®"""
    if not hybrid_retrieval:
        return jsonify({'error': 'æ··åˆæ£€ç´¢ç³»ç»Ÿæœªåˆå§‹åŒ–'}), 400
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'ç¼ºå°‘é…ç½®æ•°æ®'}), 400
        
        # æ›´æ–°é…ç½®
        hybrid_retrieval.update_config(**data)
        
        # è·å–æ›´æ–°åçš„é…ç½®
        stats = hybrid_retrieval.get_stats()
        
        return jsonify({
            'success': True,
            'message': 'é…ç½®æ›´æ–°æˆåŠŸ',
            'config': stats.get('config', {}),
            'updated_keys': list(data.keys())
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/hybrid/stats', methods=['GET'])
def get_hybrid_stats():
    """è·å–æ··åˆæ£€ç´¢ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    if not hybrid_retrieval:
        return jsonify({'error': 'æ··åˆæ£€ç´¢ç³»ç»Ÿæœªåˆå§‹åŒ–'}), 400
    
    try:
        stats = hybrid_retrieval.get_stats()
        health = hybrid_retrieval.health_check()
        
        return jsonify({
            'success': True,
            'stats': stats,
            'health': health,
            'timestamp': int(time.time())
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/hybrid/test', methods=['POST'])
def test_hybrid_retrieval():
    """æµ‹è¯•æ··åˆæ£€ç´¢ç³»ç»Ÿ"""
    if not hybrid_retrieval:
        return jsonify({'error': 'æ··åˆæ£€ç´¢ç³»ç»Ÿæœªåˆå§‹åŒ–'}), 400
    
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        strategy = data.get('strategy', 'hybrid_parallel')
        
        if not query:
            return jsonify({'error': 'æŸ¥è¯¢ä¸èƒ½ä¸ºç©º'}), 400
        
        # å°†å­—ç¬¦ä¸²ç­–ç•¥è½¬æ¢ä¸ºæšä¸¾
        strategy_map = {
            'fast_only': RetrievalStrategy.FAST_ONLY,
            'vertex_only': RetrievalStrategy.VERTEX_ONLY,
            'hybrid_parallel': RetrievalStrategy.HYBRID_PARALLEL,
            'adaptive': RetrievalStrategy.ADAPTIVE,
            'fallback': RetrievalStrategy.FALLBACK
        }
        
        strategy_enum = strategy_map.get(strategy, RetrievalStrategy.HYBRID_PARALLEL)
        
        # æ‰§è¡Œæµ‹è¯•æ£€ç´¢
        start_time = time.time()
        results = hybrid_search(
            query=query,
            hybrid_retrieval=hybrid_retrieval,
            strategy=strategy_enum
        )
        elapsed_time = time.time() - start_time
        
        return jsonify({
            'success': True,
            'query': query,
            'strategy': strategy,
            'results': results,
            'elapsed_time': elapsed_time,
            'result_count': len(results),
            'stats': hybrid_retrieval.get_stats()
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print("ğŸš€ Starting RAG API Server (Fast Boot Mode)...")
    start_time = time.time()
    
    # åˆå§‹åŒ–GCS
    print("ğŸ”§ åˆå§‹åŒ–GCS...")
    if not init_gcs():
        print("âŒ Failed to initialize GCS. Exiting...")
        sys.exit(1)

    # åˆå§‹åŒ–Vertex AI
    print("ğŸ”§ åˆå§‹åŒ–Vertex AI...")
    if not init_vertex_ai():
        print("âŒ Failed to initialize Vertex AI. Exiting...")
        sys.exit(1)
    
    # è·³è¿‡å‘é‡æœç´¢åˆå§‹åŒ–ï¼Œä½¿ç”¨æœ¬åœ°ç›¸ä¼¼åº¦æœç´¢
    print("âš¡ è·³è¿‡å‘é‡æœç´¢åˆå§‹åŒ–ï¼Œä½¿ç”¨æœ¬åœ°ç›¸ä¼¼åº¦æœç´¢...")
    
    # åˆå§‹åŒ–æ··åˆæ£€ç´¢ç³»ç»Ÿ
    init_hybrid_retrieval()
    
    # è·³è¿‡æ–‡æ¡£åŠ è½½ï¼Œä½¿ç”¨å»¶è¿ŸåŠ è½½
    print("âš¡ è·³è¿‡æ–‡æ¡£åŠ è½½ï¼Œä½¿ç”¨å»¶è¿ŸåŠ è½½æœºåˆ¶...")
    
    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
    print("ğŸ”§ åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨...")
    cache_manager = CacheManager()
    
    startup_time = time.time() - start_time
    print(f"âœ… æœåŠ¡å™¨å¯åŠ¨å®Œæˆ (è€—æ—¶: {startup_time:.2f}ç§’)")
    
    print("ğŸ“¡ API endpoints:")
    print("  GET  /health - Health check (åŒ…å«æ··åˆæ£€ç´¢ç³»ç»ŸçŠ¶æ€)")
    print("  POST /chat - Send message (ä½¿ç”¨æ··åˆæ£€ç´¢ç³»ç»Ÿ)")
    print("  POST /upload - Upload file")
    print("  GET  /files - Get uploaded files")
    print("  GET  /upload/<id>/status - Get upload status")
    print("  GET  /files/<id>/preview - Preview file content")
    print("  GET  /files/<id>/chunks - Get file chunks")
    print("  GET  /files/<id>/download - Download file from GCS")
    print("  DELETE /files/<id>/delete - Delete file from GCS")
    print("  ğŸ“Š æ··åˆæ£€ç´¢ç³»ç»Ÿç®¡ç†:")
    print("    GET  /hybrid/config - è·å–æ··åˆæ£€ç´¢é…ç½®")
    print("    POST /hybrid/config - æ›´æ–°æ··åˆæ£€ç´¢é…ç½®")
    print("    GET  /hybrid/stats - è·å–æ··åˆæ£€ç´¢ç»Ÿè®¡")
    print("    POST /hybrid/test - æµ‹è¯•æ··åˆæ£€ç´¢ç³»ç»Ÿ")
    
    print(f"ğŸŒ æœåŠ¡å™¨è¿è¡Œåœ¨ http://localhost:8080")
    print(f"âš¡ æ–‡æ¡£å°†åœ¨é¦–æ¬¡APIè°ƒç”¨æ—¶è‡ªåŠ¨åŠ è½½")
    
    app.run(host='0.0.0.0', port=8080, debug=True) 