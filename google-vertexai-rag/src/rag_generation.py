import vertexai
from vertexai.generative_models import GenerativeModel
import time
import re
import html
import logging
try:
    from .prompt_templates import (
        get_prompt_template, 
        SECURITY_KEYWORDS, 
        PROFESSIONAL_FIELDS,
        SUPPORTED_LANGUAGES
    )
except ImportError:
    from prompt_templates import (
        get_prompt_template, 
        SECURITY_KEYWORDS, 
        PROFESSIONAL_FIELDS,
        SUPPORTED_LANGUAGES
    )
from datetime import datetime

def sanitize_input(text: str) -> str:
    """
    å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œå®‰å…¨è¿‡æ»¤å’Œæ¸…ç†
    
    Args:
        text: åŸå§‹è¾“å…¥æ–‡æœ¬
        
    Returns:
        str: æ¸…ç†åçš„å®‰å…¨æ–‡æœ¬
    """
    if not text:
        return ""
    
    # è®°å½•åŸå§‹è¾“å…¥ç”¨äºå®‰å…¨å®¡è®¡
    if len(text) > 100:  # åªè®°å½•è¾ƒé•¿çš„è¾“å…¥
        logging.info(f"[å®‰å…¨å®¡è®¡] è¾“å…¥é•¿åº¦: {len(text)} å­—ç¬¦")
    
    # ç§»é™¤æ½œåœ¨çš„æ¶æ„è„šæœ¬æ ‡ç­¾ï¼ˆåœ¨HTMLç¼–ç ä¹‹å‰ï¼‰
    text = re.sub(r'<script[^>]*>.*?</script>', '[FILTERED]', text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'<iframe[^>]*>.*?</iframe>', '[FILTERED]', text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'<object[^>]*>.*?</object>', '[FILTERED]', text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'<embed[^>]*>.*?</embed>', '[FILTERED]', text, flags=re.IGNORECASE | re.DOTALL)
    
    # ç§»é™¤javascriptåè®®
    text = re.sub(r'javascript:', '[FILTERED]', text, flags=re.IGNORECASE)
    
    # ç§»é™¤alertç­‰å±é™©å‡½æ•°
    text = re.sub(r'\balert\s*\(', '[FILTERED]', text, flags=re.IGNORECASE)
    text = re.sub(r'\beval\s*\(', '[FILTERED]', text, flags=re.IGNORECASE)
    text = re.sub(r'\bonerror\s*=', '[FILTERED]', text, flags=re.IGNORECASE)
    text = re.sub(r'\bonload\s*=', '[FILTERED]', text, flags=re.IGNORECASE)
    
    # HTMLå®ä½“ç¼–ç ï¼Œé˜²æ­¢XSSæ”»å‡»
    text = html.escape(text)
    
    # ç§»é™¤æ½œåœ¨çš„SQLæ³¨å…¥æ¨¡å¼
    sql_patterns = [
        r'(\b(union|select|insert|update|delete|drop|create|alter|exec|execute)\b)',
        r'(\b(admin|root|password|passwd|user)\b)',
        r'(\b(or|and)\s+\d+\s*=\s*\d+)',
        r'(\b(union|select).*?from)',
        r'(\b(insert|update|delete).*?into)',
        r'(\bselect\s+\*.*?from)',
        r'(\bwhere\s+.*?=.*?\d+)',
        r'(\bfrom\s+\w+)',
        r'(\bwhere\s+\w+\s*=)'
    ]
    
    for pattern in sql_patterns:
        text = re.sub(pattern, '[FILTERED]', text, flags=re.IGNORECASE)
    
    # ç§»é™¤æ½œåœ¨çš„å‘½ä»¤æ³¨å…¥æ¨¡å¼
    cmd_patterns = [
        r'(\b(cmd|command|exec|system|eval|os\.|subprocess\.)\b)',
        r'(\b(rm\s+-rf|del|format|shutdown|reboot)\b)',
        r'(\b(wget|curl|nc|telnet|ssh|ftp)\b)',
        r'(\b(echo|cat|ls|dir|pwd|whoami)\b)'
    ]
    
    for pattern in cmd_patterns:
        text = re.sub(pattern, '[FILTERED]', text, flags=re.IGNORECASE)
    
    # ç§»é™¤æ½œåœ¨çš„è·¯å¾„éå†æ”»å‡»
    text = re.sub(r'\.\./', '[FILTERED]', text)
    text = re.sub(r'\.\.\\', '[FILTERED]', text)
    
    # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé˜²æ­¢è¿‡é•¿çš„æ¶æ„è¾“å…¥
    max_length = 10000
    if len(text) > max_length:
        text = text[:max_length] + "...[æˆªæ–­]"
    
    return text

def validate_query_safety(query: str) -> tuple[bool, str]:
    """
    éªŒè¯æŸ¥è¯¢çš„å®‰å…¨æ€§ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å®‰å…¨è§„åˆ™
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        
    Returns:
        tuple: (æ˜¯å¦å®‰å…¨, é”™è¯¯ä¿¡æ¯)
    """
    if not query or not query.strip():
        return False, "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º"
    
    # æ£€æŸ¥æŸ¥è¯¢é•¿åº¦
    if len(query) > 2000:
        return False, "æŸ¥è¯¢å†…å®¹è¿‡é•¿ï¼Œè¯·ç®€åŒ–é—®é¢˜"
    
    # ä½¿ç”¨ä¼˜åŒ–çš„å®‰å…¨å…³é”®è¯æ£€æµ‹
    query_lower = query.lower()
    
    # æ£€æµ‹å„ç±»æ¶æ„å…³é”®è¯
    for category, keywords in SECURITY_KEYWORDS.items():
        for keyword in keywords:
            if keyword in query_lower:
                category_names = {
                    "illegal": "è¿æ³•å†…å®¹",
                    "harmful": "æœ‰å®³å†…å®¹", 
                    "inappropriate": "ä¸å½“å†…å®¹",
                    "system": "ç³»ç»Ÿä¿¡æ¯",
                    "privacy": "éšç§ä¿¡æ¯",
                    "professional": "ä¸“ä¸šæœåŠ¡"
                }
                logging.warning(f"[å®‰å…¨å®¡è®¡] æ£€æµ‹åˆ°{category_names.get(category, 'æ•æ„Ÿå†…å®¹')}: {keyword} in query: {query[:50]}...")
                return False, f"æŸ¥è¯¢æ¶‰åŠ{category_names.get(category, 'æ•æ„Ÿå†…å®¹')}: {keyword}"
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šçš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¯èƒ½çš„ç¼–ç æ”»å‡»ï¼‰
    special_char_ratio = len(re.findall(r'[^\w\s\u4e00-\u9fff]', query)) / len(query)
    if special_char_ratio > 0.3:
        return False, "æŸ¥è¯¢åŒ…å«è¿‡å¤šç‰¹æ®Šå­—ç¬¦"
    
    # æ£€æµ‹é‡å¤å­—ç¬¦æ¨¡å¼ï¼ˆå¯èƒ½çš„DoSæ”»å‡»ï¼‰
    if len(query) > 10:
        char_counts = {}
        for char in query:
            char_counts[char] = char_counts.get(char, 0) + 1
        
        for char, count in char_counts.items():
            if count / len(query) > 0.5:  # å•ä¸ªå­—ç¬¦å æ¯”è¶…è¿‡50%
                return False, f"æ£€æµ‹åˆ°å¼‚å¸¸å­—ç¬¦æ¨¡å¼: {char}"
    
    return True, ""

def create_safe_prompt(prompt_template: str, **kwargs) -> str:
    """
    åˆ›å»ºå®‰å…¨çš„promptï¼Œå¯¹æ‰€æœ‰è¾“å…¥è¿›è¡Œå®‰å…¨å¤„ç†
    
    Args:
        prompt_template: promptæ¨¡æ¿
        **kwargs: æ¨¡æ¿å‚æ•°
        
    Returns:
        str: å®‰å…¨çš„prompt
    """
    # å¯¹æ‰€æœ‰è¾“å…¥å‚æ•°è¿›è¡Œå®‰å…¨æ¸…ç†
    safe_kwargs = {}
    for key, value in kwargs.items():
        if isinstance(value, str):
            safe_kwargs[key] = sanitize_input(value)
        elif isinstance(value, list):
            safe_kwargs[key] = [sanitize_input(str(item)) for item in value]
        else:
            safe_kwargs[key] = str(value)
    
    # ä½¿ç”¨å®‰å…¨çš„å‚æ•°å¡«å……æ¨¡æ¿
    try:
        safe_prompt = prompt_template.format(**safe_kwargs)
        
        # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
        if len(safe_prompt) > 50000:  # é™åˆ¶promptæ€»é•¿åº¦
            print("[å®‰å…¨] Promptè¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­")
            safe_prompt = safe_prompt[:50000] + "...[æˆªæ–­]"
        
        return safe_prompt
    except Exception as e:
        print(f"[å®‰å…¨] Promptæ¨¡æ¿å¡«å……å¤±è´¥: {e}")
        return "ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•"

def monitor_prompt_behavior(prompt: str, query: str) -> bool:
    """
    ç›‘æ§promptè¡Œä¸ºï¼Œæ£€æµ‹æ½œåœ¨çš„å¼‚å¸¸æ¨¡å¼
    
    Args:
        prompt: ç”Ÿæˆçš„prompt
        query: åŸå§‹æŸ¥è¯¢
        
    Returns:
        bool: æ˜¯å¦æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸º
    """
    # æ£€æµ‹é‡å¤å­—ç¬¦æ¨¡å¼ï¼ˆå¯èƒ½çš„DoSæ”»å‡»ï¼‰
    if len(prompt) > 0:
        char_counts = {}
        for char in prompt:
            char_counts[char] = char_counts.get(char, 0) + 1
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å­—ç¬¦å æ¯”è¿‡é«˜
        for char, count in char_counts.items():
            if count / len(prompt) > 0.5:  # å•ä¸ªå­—ç¬¦å æ¯”è¶…è¿‡50%
                print(f"[å®‰å…¨] æ£€æµ‹åˆ°å¼‚å¸¸å­—ç¬¦æ¨¡å¼: {char} å æ¯” {count/len(prompt):.2%}")
                return True
    
    # æ£€æµ‹æ½œåœ¨çš„promptæ³¨å…¥æ¨¡å¼
    injection_patterns = [
        r'ignore\s+previous\s+instructions',
        r'forget\s+everything',
        r'you\s+are\s+now',
        r'act\s+as\s+if',
        r'pretend\s+to\s+be',
        r'system\s+prompt',
        r'ignore\s+above',
        r'disregard\s+previous'
    ]
    
    prompt_lower = prompt.lower()
    for pattern in injection_patterns:
        if re.search(pattern, prompt_lower):
            print(f"[å®‰å…¨] æ£€æµ‹åˆ°æ½œåœ¨çš„promptæ³¨å…¥æ¨¡å¼: {pattern}")
            return True
    
    # æ£€æµ‹è¿‡é•¿çš„æŸ¥è¯¢ï¼ˆå¯èƒ½çš„èµ„æºæ¶ˆè€—æ”»å‡»ï¼‰
    if len(query) > 5000:
        print(f"[å®‰å…¨] æŸ¥è¯¢è¿‡é•¿: {len(query)} å­—ç¬¦")
        return True
    
    return False

def detect_professional_field(query: str) -> str:
    """
    æ£€æµ‹æŸ¥è¯¢æ˜¯å¦æ¶‰åŠä¸“ä¸šé¢†åŸŸ
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        
    Returns:
        str: ä¸“ä¸šé¢†åŸŸç±»å‹ï¼Œå¦‚æœä¸æ¶‰åŠåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    query_lower = query.lower()
    
    # åŒ»ç–—ç›¸å…³å…³é”®è¯
    medical_keywords = ["ç—‡çŠ¶", "ç–¾ç—…", "æ²»ç–—", "è¯ç‰©", "è¯Šæ–­", "åŒ»é™¢", "åŒ»ç”Ÿ", "æ‚£è€…", "ç—…æƒ…", "å¤´ç—›", "å‘çƒ§", "æ„Ÿå†’", "å’³å—½", "ç–¼ç—›", "ä¸èˆ’æœ", "éš¾å—"]
    if any(keyword in query_lower for keyword in medical_keywords):
        return "medical"
    
    # æ³•å¾‹ç›¸å…³å…³é”®è¯
    legal_keywords = ["æ³•å¾‹", "æ³•è§„", "åˆåŒ", "è¯‰è®¼", "å¾‹å¸ˆ", "æ³•é™¢", "åˆ¤å†³", "æƒåˆ©", "ä¹‰åŠ¡"]
    if any(keyword in query_lower for keyword in legal_keywords):
        return "legal"
    
    # é‡‘èç›¸å…³å…³é”®è¯
    financial_keywords = ["æŠ•èµ„", "ç†è´¢", "è‚¡ç¥¨", "åŸºé‡‘", "ä¿é™©", "è´·æ¬¾", "è´¢åŠ¡", "ç¨åŠ¡", "ç»æµ"]
    if any(keyword in query_lower for keyword in financial_keywords):
        return "financial"
    
    # å¿ƒç†ç›¸å…³å…³é”®è¯
    psychological_keywords = ["å¿ƒç†", "æƒ…ç»ª", "æŠ‘éƒ", "ç„¦è™‘", "å’¨è¯¢", "æ²»ç–—", "å‹åŠ›", "å¿ƒç†å¥åº·"]
    if any(keyword in query_lower for keyword in psychological_keywords):
        return "psychological"
    
    return ""

def create_security_rejection_response(rejection_reason: str) -> str:
    """
    åˆ›å»ºå®‰å…¨æ‹’ç»å“åº”
    
    Args:
        rejection_reason: æ‹’ç»åŸå› 
        
    Returns:
        str: æ ¼å¼åŒ–çš„æ‹’ç»å“åº”
    """
    template = get_prompt_template("security_rejection")
    return template.format(rejection_reason=rejection_reason)

def create_professional_advice_response(answer: str, professional_field: str) -> str:
    """
    åˆ›å»ºä¸“ä¸šå»ºè®®å“åº”
    
    Args:
        answer: åŸå§‹å›ç­”
        professional_field: ä¸“ä¸šé¢†åŸŸ
        
    Returns:
        str: æ ¼å¼åŒ–çš„ä¸“ä¸šå»ºè®®å“åº”
    """
    if professional_field in PROFESSIONAL_FIELDS:
        field_info = PROFESSIONAL_FIELDS[professional_field]
        template = get_prompt_template("professional_advice")
        return template.format(
            answer=answer,
            professional_field=field_info["field"],
            professional_type=field_info["type"],
            source_identifier="ğŸ§  *åŸºäºAIçŸ¥è¯†*"
        )
    return answer

def generate_answer_with_llm(query: str, retrieved_chunks: list[str], sources: list[dict] = None, similarity_threshold: float = 0.6) -> dict:
    """
    ç»“åˆæ£€ç´¢åˆ°çš„æ–‡æœ¬å’Œç”¨æˆ·æŸ¥è¯¢ï¼Œä½¿ç”¨LLMç”Ÿæˆå›ç­”ã€‚
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æœ¬å—
        sources: æ£€ç´¢ç»“æœçš„æºä¿¡æ¯ï¼ˆåŒ…å«ç›¸ä¼¼åº¦ç­‰ï¼‰
        similarity_threshold: ç›¸å…³æ€§é˜ˆå€¼ï¼Œä½äºæ­¤å€¼å°†ä½¿ç”¨åŸºç¡€çŸ¥è¯†å›ç­”
        
    Returns:
        dict: åŒ…å«å›ç­”ã€ç­”æ¡ˆæ¥æºã€å¯ä¿¡åº¦ç­‰ä¿¡æ¯
    """
    # å®‰å…¨éªŒè¯
    is_safe, error_msg = validate_query_safety(query)
    if not is_safe:
        print(f"[å®‰å…¨] æŸ¥è¯¢å®‰å…¨æ£€æŸ¥å¤±è´¥: {error_msg}")
        security_response = create_security_rejection_response(error_msg)
        return {
            "answer": security_response,
            "source": "security_error",
            "confidence": 0,
            "processing_time": 0,
            "use_rag": False,
            "use_hybrid": False,
            "max_similarity": 0
        }
    
    print(f"[LLM] å¼€å§‹ç”Ÿæˆå›ç­”ï¼Œquery: {query}")
    print(f"[LLM] ä¸Šä¸‹æ–‡å—æ•°: {len(retrieved_chunks)}ï¼Œæ€»é•¿åº¦: {sum(len(t) for t in retrieved_chunks)}")
    
    # æ£€æŸ¥æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§
    use_rag = False
    use_hybrid = False
    
    if sources:
        # è®¡ç®—æœ€é«˜ç›¸ä¼¼åº¦
        max_similarity = max(source.get('similarity', 0) for source in sources)
        print(f"[LLM] æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}ï¼Œé˜ˆå€¼: {similarity_threshold}")
        
        # åˆ†å±‚ç›¸ä¼¼åº¦ç­–ç•¥
        if max_similarity >= 0.85:
            use_rag = True
            print(f"[LLM] âœ… é«˜ç›¸ä¼¼åº¦ (â‰¥85%)ï¼Œä½¿ç”¨çº¯RAGæ£€ç´¢å›ç­”")
        elif max_similarity >= similarity_threshold:
            use_hybrid = True
            print(f"[LLM] ğŸ”„ ä¸­ç­‰ç›¸ä¼¼åº¦ ({similarity_threshold:.0%}-85%)ï¼Œä½¿ç”¨æ··åˆæ¨¡å¼å›ç­”")
        else:
            print(f"[LLM] âŒ ä½ç›¸ä¼¼åº¦ (<{similarity_threshold:.0%})ï¼Œä½¿ç”¨åŸºç¡€çŸ¥è¯†å›ç­”")
    
    if retrieved_chunks and use_rag:
        print(f"[LLM] ä¸Šä¸‹æ–‡é¢„è§ˆ: {retrieved_chunks[0][:100]} ...")
    
    start = time.time()
    model = GenerativeModel('gemini-2.0-flash-001')

    # æ ¹æ®ç›¸ä¼¼åº¦é€‰æ‹©ä¸åŒçš„å›ç­”ç­–ç•¥ï¼Œä½¿ç”¨ä¼˜åŒ–çš„promptæ¨¡æ¿
    if use_rag and retrieved_chunks:
        # é«˜ç›¸ä¼¼åº¦ï¼šçº¯RAGæ¨¡å¼
        context = "\n\n".join(retrieved_chunks)
        prompt_template = get_prompt_template("rag_high")
        prompt = create_safe_prompt(prompt_template, context=context, query=query)
        answer_source = "rag"
        confidence = max_similarity
        
    elif use_hybrid and retrieved_chunks:
        # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šæ··åˆæ¨¡å¼
        context = "\n\n".join(retrieved_chunks)
        prompt_template = get_prompt_template("hybrid")
        prompt = create_safe_prompt(prompt_template, context=context, query=query)
        answer_source = "hybrid"
        confidence = max_similarity
        
    else:
        # ä½ç›¸ä¼¼åº¦ï¼šåŸºç¡€çŸ¥è¯†æ¨¡å¼
        prompt_template = get_prompt_template("knowledge")
        prompt = create_safe_prompt(prompt_template, query=query)
        answer_source = "knowledge"
        confidence = 0.5  # åŸºç¡€çŸ¥è¯†å›ç­”ç»™äºˆä¸­ç­‰ç½®ä¿¡åº¦
    
    mode_name = "çº¯RAG" if use_rag else ("æ··åˆæ¨¡å¼" if use_hybrid else "åŸºç¡€çŸ¥è¯†")
    print(f"[LLM] ä½¿ç”¨{mode_name}ç”Ÿæˆå›ç­”")
    
    # è¡Œä¸ºç›‘æ§
    if monitor_prompt_behavior(prompt, query):
        print("[å®‰å…¨] æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸ºï¼Œè¿”å›å®‰å…¨å“åº”")
        return {
            "answer": "æŠ±æ­‰ï¼Œæ£€æµ‹åˆ°å¼‚å¸¸è¾“å…¥æ¨¡å¼ã€‚è¯·é‡æ–°è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚",
            "source": "security_monitor",
            "confidence": 0,
            "processing_time": time.time() - start,
            "use_rag": False,
            "use_hybrid": False,
            "max_similarity": 0
        }
    
    try:
        response = model.generate_content(prompt)
        
        if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
            answer = response.candidates[0].content.parts[0].text
            processing_time = time.time() - start
            
            # æ£€æµ‹ä¸“ä¸šé¢†åŸŸå¹¶æ·»åŠ ç›¸åº”æé†’
            professional_field = detect_professional_field(query)
            if professional_field and answer_source == "knowledge":
                answer = create_professional_advice_response(answer, professional_field)
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "answer": answer,
                "source": answer_source,
                "confidence": confidence,
                "processing_time": processing_time,
                "use_rag": use_rag or use_hybrid,
                "use_hybrid": use_hybrid,
                "max_similarity": max(source.get('similarity', 0) for source in sources) if sources else 0,
                "professional_field": professional_field
            }
            
            print(f"[LLM] ç”Ÿæˆå›ç­”: {answer[:200]} ...")
            print(f"[LLM] å›ç­”æ¥æº: {answer_source}")
            print(f"[LLM] ç½®ä¿¡åº¦: {confidence:.3f}")
            print(f"[LLM] ç”Ÿæˆè€—æ—¶: {processing_time:.2f}ç§’")
            
            return result
        else:
            print("[LLM] LLMå“åº”ä¸­æ²¡æœ‰å¯ç”¨çš„æ–‡æœ¬å†…å®¹")
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›ç­”ã€‚",
                "source": "error",
                "confidence": 0,
                "processing_time": time.time() - start,
                "use_rag": False,
                "use_hybrid": False,
                "max_similarity": 0
            }
    except Exception as e:
        print(f"[LLM] ç”Ÿæˆå›ç­”å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return {
            "answer": "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚",
            "source": "error",
            "confidence": 0,
            "processing_time": time.time() - start,
            "use_rag": False,
            "use_hybrid": False,
            "max_similarity": 0
        }

# ä¿ç•™åŸæœ‰çš„ç®€å•ç‰ˆæœ¬å‡½æ•°ï¼Œç”¨äºå‘åå…¼å®¹
def generate_answer_with_llm_simple(query: str, retrieved_chunks: list[str]) -> str:
    """
    ç®€å•ç‰ˆæœ¬çš„ç”Ÿæˆå‡½æ•°ï¼Œä¿æŒå‘åå…¼å®¹
    """
    result = generate_answer_with_llm(query, retrieved_chunks)
    return result["answer"]

# é…ç½®å®‰å…¨æ—¥å¿—
def setup_security_logging():
    """é…ç½®å®‰å…¨æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('security.log'),
            logging.StreamHandler()
        ]
    )

if __name__ == "__main__":
    setup_security_logging()
    pass 
