import vertexai
from vertexai.generative_models import GenerativeModel
import time

def generate_answer_with_llm(query: str, retrieved_chunks: list[str], sources: list[dict] = None, similarity_threshold: float = 0.6) -> dict:
    """
    ç»“åˆæ£€ç´¢åˆ°çš„æ–‡æœ¬å’Œç”¨æˆ·æŸ¥è¯¢ï¼Œä½¿ç”¨LLMç”Ÿæˆå›ç­”ã€‚
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æœ¬å—
        sources: æ£€ç´¢ç»“æœçš„æºä¿¡æ¯ï¼ˆåŒ…å«ç›¸ä¼¼åº¦ç­‰ï¼‰
        similarity_threshold: ç›¸å…³æ€§é˜ˆå€¼ï¼Œä½äºæ­¤å€¼å°†ä½¿ç”¨åŸºç¡€çŸ¥è¯†å›ç­”
        
    Returns:
        dict: åŒ…å«å›ç­”ã€ç­”æ¡ˆæ¥æºã€å¯ä¿¡åº¦ç­‰ä¿¡æ¯
    """
    print(f"[LLM] å¼€å§‹ç”Ÿæˆå›ç­”ï¼Œquery: {query}")
    print(f"[LLM] ä¸Šä¸‹æ–‡å—æ•°: {len(retrieved_chunks)}ï¼Œæ€»é•¿åº¦: {sum(len(t) for t in retrieved_chunks)}")
    
    # æ£€æŸ¥æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§
    use_rag = False
    use_hybrid = False
    
    if sources:
        # è®¡ç®—æœ€é«˜ç›¸ä¼¼åº¦
        max_similarity = max(source.get('similarity', 0) for source in sources)
        print(f"[LLM] æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}ï¼Œé˜ˆå€¼: {similarity_threshold}")
        
        # åˆ†å±‚ç›¸ä¼¼åº¦ç­–ç•¥
        if max_similarity >= 0.85:
            use_rag = True
            print(f"[LLM] âœ… é«˜ç›¸ä¼¼åº¦ (â‰¥85%)ï¼Œä½¿ç”¨çº¯RAGæ£€ç´¢å›ç­”")
        elif max_similarity >= similarity_threshold:
            use_hybrid = True
            print(f"[LLM] ğŸ”„ ä¸­ç­‰ç›¸ä¼¼åº¦ ({similarity_threshold:.0%}-85%)ï¼Œä½¿ç”¨æ··åˆæ¨¡å¼å›ç­”")
        else:
            print(f"[LLM] âŒ ä½ç›¸ä¼¼åº¦ (<{similarity_threshold:.0%})ï¼Œä½¿ç”¨åŸºç¡€çŸ¥è¯†å›ç­”")
    
    if retrieved_chunks and use_rag:
        print(f"[LLM] ä¸Šä¸‹æ–‡é¢„è§ˆ: {retrieved_chunks[0][:100]} ...")
    
    start = time.time()
    model = GenerativeModel('gemini-2.0-flash-001')

    # æ ¹æ®ç›¸ä¼¼åº¦é€‰æ‹©ä¸åŒçš„å›ç­”ç­–ç•¥
    if use_rag and retrieved_chunks:
        # é«˜ç›¸ä¼¼åº¦ï¼šçº¯RAGæ¨¡å¼
        context = "\n\n".join(retrieved_chunks)
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ä¸ç”¨æˆ·é—®é¢˜é«˜åº¦ç›¸å…³ï¼Œè¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ã€‚

**æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼š**
{context}

**ç”¨æˆ·é—®é¢˜ï¼š** {query}

**å›ç­”è¦æ±‚ï¼š**
- åŸºäºæ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”
- å›ç­”è¦æ¸…æ™°ç®€æ´ï¼Œæ˜“äºç†è§£
- ä½¿ç”¨ç®€å•çš„markdownæ ¼å¼ï¼ˆåŠ ç²—ã€åˆ—è¡¨ç­‰ï¼‰
- é¿å…è¿‡é•¿çš„æ®µè½ï¼Œé€‚å½“åˆ†æ®µ
- å›ç­”ç»“å°¾æ·»åŠ ï¼š"ğŸ“– *åŸºäºæ£€ç´¢æ–‡æ¡£*"

**å›ç­”ï¼š**
"""
        answer_source = "rag"
        confidence = max_similarity
        
    elif use_hybrid and retrieved_chunks:
        # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šæ··åˆæ¨¡å¼
        context = "\n\n".join(retrieved_chunks)
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ä¸ç”¨æˆ·é—®é¢˜æœ‰ä¸€å®šç›¸å…³æ€§ï¼Œè¯·ç»“åˆæ–‡æ¡£å’ŒçŸ¥è¯†å›ç­”ã€‚

**æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼š**
{context}

**ç”¨æˆ·é—®é¢˜ï¼š** {query}

**å›ç­”è¦æ±‚ï¼š**
- å…ˆåˆ†ææ–‡æ¡£ä¸­çš„ç›¸å…³ä¿¡æ¯
- ç»“åˆåŸºç¡€çŸ¥è¯†æä¾›å®Œæ•´å‡†ç¡®çš„ç­”æ¡ˆ
- å›ç­”è¦æ¸…æ™°ç®€æ´ï¼Œåˆ†ç‚¹è¯´æ˜
- ä½¿ç”¨ç®€å•çš„markdownæ ¼å¼ï¼ˆ**åŠ ç²—**ã€- åˆ—è¡¨ç­‰ï¼‰
- é¿å…è¿‡é•¿çš„æ®µè½å’Œå¤æ‚è¡¨æ ¼
- å›ç­”ç»“å°¾æ·»åŠ ï¼š"ğŸ”„ *ç»“åˆæ–‡æ¡£å’ŒçŸ¥è¯†*"

**å›ç­”ï¼š**
"""
        answer_source = "hybrid"
        confidence = max_similarity
        
    else:
        # ä½ç›¸ä¼¼åº¦ï¼šåŸºç¡€çŸ¥è¯†æ¨¡å¼
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**ç”¨æˆ·é—®é¢˜ï¼š** {query}

**å›ç­”è¦æ±‚ï¼š**
- æä¾›å‡†ç¡®ã€æ˜“æ‡‚çš„è§£é‡Š
- åˆ†ç‚¹è¯´æ˜å…³é”®ä¿¡æ¯
- ä½¿ç”¨ç®€å•çš„markdownæ ¼å¼ï¼ˆ**åŠ ç²—**ã€- åˆ—è¡¨ç­‰ï¼‰
- å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé¿å…å†—é•¿
- å¦‚æœæ˜¯ä¸“ä¸šé—®é¢˜ï¼Œæé†’ç”¨æˆ·å’¨è¯¢ä¸“å®¶
- å›ç­”ç»“å°¾æ·»åŠ ï¼š"ğŸ§  *åŸºäºAIçŸ¥è¯†*"

**å›ç­”ï¼š**
"""
        answer_source = "knowledge"
        confidence = 0.5  # åŸºç¡€çŸ¥è¯†å›ç­”ç»™äºˆä¸­ç­‰ç½®ä¿¡åº¦
    
    mode_name = "çº¯RAG" if use_rag else ("æ··åˆæ¨¡å¼" if use_hybrid else "åŸºç¡€çŸ¥è¯†")
    print(f"[LLM] ä½¿ç”¨{mode_name}ç”Ÿæˆå›ç­”")
    
    try:
        response = model.generate_content(prompt)
        
        if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
            answer = response.candidates[0].content.parts[0].text
            processing_time = time.time() - start
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "answer": answer,
                "source": answer_source,
                "confidence": confidence,
                "processing_time": processing_time,
                "use_rag": use_rag or use_hybrid,
                "use_hybrid": use_hybrid,
                "max_similarity": max(source.get('similarity', 0) for source in sources) if sources else 0
            }
            
            print(f"[LLM] ç”Ÿæˆå›ç­”: {answer[:200]} ...")
            print(f"[LLM] å›ç­”æ¥æº: {answer_source}")
            print(f"[LLM] ç½®ä¿¡åº¦: {confidence:.3f}")
            print(f"[LLM] ç”Ÿæˆè€—æ—¶: {processing_time:.2f}ç§’")
            
            return result
        else:
            print("[LLM] LLMå“åº”ä¸­æ²¡æœ‰å¯ç”¨çš„æ–‡æœ¬å†…å®¹")
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›ç­”ã€‚",
                "source": "error",
                "confidence": 0,
                "processing_time": time.time() - start,
                "use_rag": False,
                "use_hybrid": False,
                "max_similarity": 0
            }
    except Exception as e:
        print(f"[LLM] ç”Ÿæˆå›ç­”å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return {
            "answer": "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚",
            "source": "error",
            "confidence": 0,
            "processing_time": time.time() - start,
            "use_rag": False,
            "use_hybrid": False,
            "max_similarity": 0
        }

# ä¿ç•™åŸæœ‰çš„ç®€å•ç‰ˆæœ¬å‡½æ•°ï¼Œç”¨äºå‘åå…¼å®¹
def generate_answer_with_llm_simple(query: str, retrieved_chunks: list[str]) -> str:
    """
    ç®€å•ç‰ˆæœ¬çš„ç”Ÿæˆå‡½æ•°ï¼Œä¿æŒå‘åå…¼å®¹
    """
    result = generate_answer_with_llm(query, retrieved_chunks)
    return result["answer"]

if __name__ == "__main__":
    pass 
