import vertexai
from vertexai.generative_models import GenerativeModel
import time
import re
import html
import logging
from datetime import datetime

def sanitize_input(text: str) -> str:
    """
    å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œå®‰å…¨è¿‡æ»¤å’Œæ¸…ç†
    
    Args:
        text: åŸå§‹è¾“å…¥æ–‡æœ¬
        
    Returns:
        str: æ¸…ç†åçš„å®‰å…¨æ–‡æœ¬
    """
    if not text:
        return ""
    
    # è®°å½•åŸå§‹è¾“å…¥ç”¨äºå®‰å…¨å®¡è®¡
    if len(text) > 100:  # åªè®°å½•è¾ƒé•¿çš„è¾“å…¥
        logging.info(f"[å®‰å…¨å®¡è®¡] è¾“å…¥é•¿åº¦: {len(text)} å­—ç¬¦")
    
    # ç§»é™¤æ½œåœ¨çš„æ¶æ„è„šæœ¬æ ‡ç­¾ï¼ˆåœ¨HTMLç¼–ç ä¹‹å‰ï¼‰
    text = re.sub(r'<script[^>]*>.*?</script>', '[FILTERED]', text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'<iframe[^>]*>.*?</iframe>', '[FILTERED]', text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'<object[^>]*>.*?</object>', '[FILTERED]', text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'<embed[^>]*>.*?</embed>', '[FILTERED]', text, flags=re.IGNORECASE | re.DOTALL)
    
    # ç§»é™¤javascriptåè®®
    text = re.sub(r'javascript:', '[FILTERED]', text, flags=re.IGNORECASE)
    
    # ç§»é™¤alertç­‰å±é™©å‡½æ•°
    text = re.sub(r'\balert\s*\(', '[FILTERED]', text, flags=re.IGNORECASE)
    text = re.sub(r'\beval\s*\(', '[FILTERED]', text, flags=re.IGNORECASE)
    text = re.sub(r'\bonerror\s*=', '[FILTERED]', text, flags=re.IGNORECASE)
    text = re.sub(r'\bonload\s*=', '[FILTERED]', text, flags=re.IGNORECASE)
    
    # HTMLå®ä½“ç¼–ç ï¼Œé˜²æ­¢XSSæ”»å‡»
    text = html.escape(text)
    
    # ç§»é™¤æ½œåœ¨çš„SQLæ³¨å…¥æ¨¡å¼
    sql_patterns = [
        r'(\b(union|select|insert|update|delete|drop|create|alter|exec|execute)\b)',
        r'(\b(admin|root|password|passwd|user)\b)',
        r'(\b(or|and)\s+\d+\s*=\s*\d+)',
        r'(\b(union|select).*?from)',
        r'(\b(insert|update|delete).*?into)',
        r'(\bselect\s+\*.*?from)',
        r'(\bwhere\s+.*?=.*?\d+)',
        r'(\bfrom\s+\w+)',
        r'(\bwhere\s+\w+\s*=)'
    ]
    
    for pattern in sql_patterns:
        text = re.sub(pattern, '[FILTERED]', text, flags=re.IGNORECASE)
    
    # ç§»é™¤æ½œåœ¨çš„å‘½ä»¤æ³¨å…¥æ¨¡å¼
    cmd_patterns = [
        r'(\b(cmd|command|exec|system|eval|os\.|subprocess\.)\b)',
        r'(\b(rm\s+-rf|del|format|shutdown|reboot)\b)',
        r'(\b(wget|curl|nc|telnet|ssh|ftp)\b)',
        r'(\b(echo|cat|ls|dir|pwd|whoami)\b)'
    ]
    
    for pattern in cmd_patterns:
        text = re.sub(pattern, '[FILTERED]', text, flags=re.IGNORECASE)
    
    # ç§»é™¤æ½œåœ¨çš„è·¯å¾„éå†æ”»å‡»
    text = re.sub(r'\.\./', '[FILTERED]', text)
    text = re.sub(r'\.\.\\', '[FILTERED]', text)
    
    # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé˜²æ­¢è¿‡é•¿çš„æ¶æ„è¾“å…¥
    max_length = 10000
    if len(text) > max_length:
        text = text[:max_length] + "...[æˆªæ–­]"
    
    return text

def validate_query_safety(query: str) -> tuple[bool, str]:
    """
    éªŒè¯æŸ¥è¯¢çš„å®‰å…¨æ€§
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        
    Returns:
        tuple: (æ˜¯å¦å®‰å…¨, é”™è¯¯ä¿¡æ¯)
    """
    if not query or not query.strip():
        return False, "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º"
    
    # æ£€æŸ¥æŸ¥è¯¢é•¿åº¦
    if len(query) > 2000:
        return False, "æŸ¥è¯¢å†…å®¹è¿‡é•¿ï¼Œè¯·ç®€åŒ–é—®é¢˜"
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„æ¶æ„å†…å®¹
    malicious_keywords = [
        'hack', 'exploit', 'vulnerability', 'bypass', 'inject',
        'sql injection', 'xss', 'csrf', 'buffer overflow',
        'privilege escalation', 'backdoor', 'trojan', 'virus',
        'malware', 'phishing', 'ddos', 'brute force'
    ]
    
    query_lower = query.lower()
    for keyword in malicious_keywords:
        if keyword in query_lower:
            logging.warning(f"[å®‰å…¨å®¡è®¡] æ£€æµ‹åˆ°æ¶æ„å…³é”®è¯: {keyword} in query: {query[:50]}...")
            return False, f"æŸ¥è¯¢åŒ…å«ä¸å½“å†…å®¹: {keyword}"
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šçš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¯èƒ½çš„ç¼–ç æ”»å‡»ï¼‰
    special_char_ratio = len(re.findall(r'[^\w\s\u4e00-\u9fff]', query)) / len(query)
    if special_char_ratio > 0.3:
        return False, "æŸ¥è¯¢åŒ…å«è¿‡å¤šç‰¹æ®Šå­—ç¬¦"
    
    return True, ""

def create_safe_prompt(prompt_template: str, **kwargs) -> str:
    """
    åˆ›å»ºå®‰å…¨çš„promptï¼Œå¯¹æ‰€æœ‰è¾“å…¥è¿›è¡Œå®‰å…¨å¤„ç†
    
    Args:
        prompt_template: promptæ¨¡æ¿
        **kwargs: æ¨¡æ¿å‚æ•°
        
    Returns:
        str: å®‰å…¨çš„prompt
    """
    # å¯¹æ‰€æœ‰è¾“å…¥å‚æ•°è¿›è¡Œå®‰å…¨æ¸…ç†
    safe_kwargs = {}
    for key, value in kwargs.items():
        if isinstance(value, str):
            safe_kwargs[key] = sanitize_input(value)
        elif isinstance(value, list):
            safe_kwargs[key] = [sanitize_input(str(item)) for item in value]
        else:
            safe_kwargs[key] = str(value)
    
    # ä½¿ç”¨å®‰å…¨çš„å‚æ•°å¡«å……æ¨¡æ¿
    try:
        safe_prompt = prompt_template.format(**safe_kwargs)
        
        # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
        if len(safe_prompt) > 50000:  # é™åˆ¶promptæ€»é•¿åº¦
            print("[å®‰å…¨] Promptè¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­")
            safe_prompt = safe_prompt[:50000] + "...[æˆªæ–­]"
        
        return safe_prompt
    except Exception as e:
        print(f"[å®‰å…¨] Promptæ¨¡æ¿å¡«å……å¤±è´¥: {e}")
        return "ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•"

def monitor_prompt_behavior(prompt: str, query: str) -> bool:
    """
    ç›‘æ§promptè¡Œä¸ºï¼Œæ£€æµ‹æ½œåœ¨çš„å¼‚å¸¸æ¨¡å¼
    
    Args:
        prompt: ç”Ÿæˆçš„prompt
        query: åŸå§‹æŸ¥è¯¢
        
    Returns:
        bool: æ˜¯å¦æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸º
    """
    # æ£€æµ‹é‡å¤å­—ç¬¦æ¨¡å¼ï¼ˆå¯èƒ½çš„DoSæ”»å‡»ï¼‰
    if len(prompt) > 0:
        char_counts = {}
        for char in prompt:
            char_counts[char] = char_counts.get(char, 0) + 1
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å­—ç¬¦å æ¯”è¿‡é«˜
        for char, count in char_counts.items():
            if count / len(prompt) > 0.5:  # å•ä¸ªå­—ç¬¦å æ¯”è¶…è¿‡50%
                print(f"[å®‰å…¨] æ£€æµ‹åˆ°å¼‚å¸¸å­—ç¬¦æ¨¡å¼: {char} å æ¯” {count/len(prompt):.2%}")
                return True
    
    # æ£€æµ‹æ½œåœ¨çš„promptæ³¨å…¥æ¨¡å¼
    injection_patterns = [
        r'ignore\s+previous\s+instructions',
        r'forget\s+everything',
        r'you\s+are\s+now',
        r'act\s+as\s+if',
        r'pretend\s+to\s+be',
        r'system\s+prompt',
        r'ignore\s+above',
        r'disregard\s+previous'
    ]
    
    prompt_lower = prompt.lower()
    for pattern in injection_patterns:
        if re.search(pattern, prompt_lower):
            print(f"[å®‰å…¨] æ£€æµ‹åˆ°æ½œåœ¨çš„promptæ³¨å…¥æ¨¡å¼: {pattern}")
            return True
    
    # æ£€æµ‹è¿‡é•¿çš„æŸ¥è¯¢ï¼ˆå¯èƒ½çš„èµ„æºæ¶ˆè€—æ”»å‡»ï¼‰
    if len(query) > 5000:
        print(f"[å®‰å…¨] æŸ¥è¯¢è¿‡é•¿: {len(query)} å­—ç¬¦")
        return True
    
    return False

def generate_answer_with_llm(query: str, retrieved_chunks: list[str], sources: list[dict] = None, similarity_threshold: float = 0.6) -> dict:
    """
    ç»“åˆæ£€ç´¢åˆ°çš„æ–‡æœ¬å’Œç”¨æˆ·æŸ¥è¯¢ï¼Œä½¿ç”¨LLMç”Ÿæˆå›ç­”ã€‚
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æœ¬å—
        sources: æ£€ç´¢ç»“æœçš„æºä¿¡æ¯ï¼ˆåŒ…å«ç›¸ä¼¼åº¦ç­‰ï¼‰
        similarity_threshold: ç›¸å…³æ€§é˜ˆå€¼ï¼Œä½äºæ­¤å€¼å°†ä½¿ç”¨åŸºç¡€çŸ¥è¯†å›ç­”
        
    Returns:
        dict: åŒ…å«å›ç­”ã€ç­”æ¡ˆæ¥æºã€å¯ä¿¡åº¦ç­‰ä¿¡æ¯
    """
    # å®‰å…¨éªŒè¯
    is_safe, error_msg = validate_query_safety(query)
    if not is_safe:
        print(f"[å®‰å…¨] æŸ¥è¯¢å®‰å…¨æ£€æŸ¥å¤±è´¥: {error_msg}")
        return {
            "answer": f"æŠ±æ­‰ï¼Œæ‚¨çš„æŸ¥è¯¢å­˜åœ¨å®‰å…¨é—®é¢˜ï¼š{error_msg}ã€‚è¯·é‡æ–°è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚",
            "source": "security_error",
            "confidence": 0,
            "processing_time": 0,
            "use_rag": False,
            "use_hybrid": False,
            "max_similarity": 0
        }
    
    print(f"[LLM] å¼€å§‹ç”Ÿæˆå›ç­”ï¼Œquery: {query}")
    print(f"[LLM] ä¸Šä¸‹æ–‡å—æ•°: {len(retrieved_chunks)}ï¼Œæ€»é•¿åº¦: {sum(len(t) for t in retrieved_chunks)}")
    
    # æ£€æŸ¥æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§
    use_rag = False
    use_hybrid = False
    
    if sources:
        # è®¡ç®—æœ€é«˜ç›¸ä¼¼åº¦
        max_similarity = max(source.get('similarity', 0) for source in sources)
        print(f"[LLM] æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}ï¼Œé˜ˆå€¼: {similarity_threshold}")
        
        # åˆ†å±‚ç›¸ä¼¼åº¦ç­–ç•¥
        if max_similarity >= 0.85:
            use_rag = True
            print(f"[LLM] âœ… é«˜ç›¸ä¼¼åº¦ (â‰¥85%)ï¼Œä½¿ç”¨çº¯RAGæ£€ç´¢å›ç­”")
        elif max_similarity >= similarity_threshold:
            use_hybrid = True
            print(f"[LLM] ğŸ”„ ä¸­ç­‰ç›¸ä¼¼åº¦ ({similarity_threshold:.0%}-85%)ï¼Œä½¿ç”¨æ··åˆæ¨¡å¼å›ç­”")
        else:
            print(f"[LLM] âŒ ä½ç›¸ä¼¼åº¦ (<{similarity_threshold:.0%})ï¼Œä½¿ç”¨åŸºç¡€çŸ¥è¯†å›ç­”")
    
    if retrieved_chunks and use_rag:
        print(f"[LLM] ä¸Šä¸‹æ–‡é¢„è§ˆ: {retrieved_chunks[0][:100]} ...")
    
    start = time.time()
    model = GenerativeModel('gemini-2.0-flash-001')

    # æ ¹æ®ç›¸ä¼¼åº¦é€‰æ‹©ä¸åŒçš„å›ç­”ç­–ç•¥
    if use_rag and retrieved_chunks:
        # é«˜ç›¸ä¼¼åº¦ï¼šçº¯RAGæ¨¡å¼
        context = "\n\n".join(retrieved_chunks)
        prompt = create_safe_prompt("""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ä¸ç”¨æˆ·é—®é¢˜é«˜åº¦ç›¸å…³ï¼Œè¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ã€‚

**æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼š**
{context}

**ç”¨æˆ·é—®é¢˜ï¼š** {query}

**å›ç­”è¦æ±‚ï¼š**
- åŸºäºæ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”
- å›ç­”è¦æ¸…æ™°ç®€æ´ï¼Œæ˜“äºç†è§£
- ä½¿ç”¨ç®€å•çš„markdownæ ¼å¼ï¼ˆåŠ ç²—ã€åˆ—è¡¨ç­‰ï¼‰
- é¿å…è¿‡é•¿çš„æ®µè½ï¼Œé€‚å½“åˆ†æ®µ
- å›ç­”ç»“å°¾æ·»åŠ ï¼š"ğŸ“– *åŸºäºæ£€ç´¢æ–‡æ¡£*"

**å›ç­”ï¼š**
""", context=context, query=query)
        answer_source = "rag"
        confidence = max_similarity
        
    elif use_hybrid and retrieved_chunks:
        # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šæ··åˆæ¨¡å¼
        context = "\n\n".join(retrieved_chunks)
        prompt = create_safe_prompt("""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ä¸ç”¨æˆ·é—®é¢˜æœ‰ä¸€å®šç›¸å…³æ€§ï¼Œè¯·ç»“åˆæ–‡æ¡£å’ŒçŸ¥è¯†å›ç­”ã€‚

**æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼š**
{context}

**ç”¨æˆ·é—®é¢˜ï¼š** {query}

**å›ç­”è¦æ±‚ï¼š**
- å…ˆåˆ†ææ–‡æ¡£ä¸­çš„ç›¸å…³ä¿¡æ¯
- ç»“åˆåŸºç¡€çŸ¥è¯†æä¾›å®Œæ•´å‡†ç¡®çš„ç­”æ¡ˆ
- å›ç­”è¦æ¸…æ™°ç®€æ´ï¼Œåˆ†ç‚¹è¯´æ˜
- ä½¿ç”¨ç®€å•çš„markdownæ ¼å¼ï¼ˆ**åŠ ç²—**ã€- åˆ—è¡¨ç­‰ï¼‰
- é¿å…è¿‡é•¿çš„æ®µè½å’Œå¤æ‚è¡¨æ ¼
- å›ç­”ç»“å°¾æ·»åŠ ï¼š"ğŸ”„ *ç»“åˆæ–‡æ¡£å’ŒçŸ¥è¯†*"

**å›ç­”ï¼š**
""", context=context, query=query)
        answer_source = "hybrid"
        confidence = max_similarity
        
    else:
        # ä½ç›¸ä¼¼åº¦ï¼šåŸºç¡€çŸ¥è¯†æ¨¡å¼
        prompt = create_safe_prompt("""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**ç”¨æˆ·é—®é¢˜ï¼š** {query}

**å›ç­”è¦æ±‚ï¼š**
- æä¾›å‡†ç¡®ã€æ˜“æ‡‚çš„è§£é‡Š
- åˆ†ç‚¹è¯´æ˜å…³é”®ä¿¡æ¯
- ä½¿ç”¨ç®€å•çš„markdownæ ¼å¼ï¼ˆ**åŠ ç²—**ã€- åˆ—è¡¨ç­‰ï¼‰
- å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé¿å…å†—é•¿
- å¦‚æœæ˜¯ä¸“ä¸šé—®é¢˜ï¼Œæé†’ç”¨æˆ·å’¨è¯¢ä¸“å®¶
- å›ç­”ç»“å°¾æ·»åŠ ï¼š"ğŸ§  *åŸºäºAIçŸ¥è¯†*"

**å›ç­”ï¼š**
""", query=query)
        answer_source = "knowledge"
        confidence = 0.5  # åŸºç¡€çŸ¥è¯†å›ç­”ç»™äºˆä¸­ç­‰ç½®ä¿¡åº¦
    
    mode_name = "çº¯RAG" if use_rag else ("æ··åˆæ¨¡å¼" if use_hybrid else "åŸºç¡€çŸ¥è¯†")
    print(f"[LLM] ä½¿ç”¨{mode_name}ç”Ÿæˆå›ç­”")
    
    # è¡Œä¸ºç›‘æ§
    if monitor_prompt_behavior(prompt, query):
        print("[å®‰å…¨] æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸ºï¼Œè¿”å›å®‰å…¨å“åº”")
        return {
            "answer": "æŠ±æ­‰ï¼Œæ£€æµ‹åˆ°å¼‚å¸¸è¾“å…¥æ¨¡å¼ã€‚è¯·é‡æ–°è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚",
            "source": "security_monitor",
            "confidence": 0,
            "processing_time": time.time() - start,
            "use_rag": False,
            "use_hybrid": False,
            "max_similarity": 0
        }
    
    try:
        response = model.generate_content(prompt)
        
        if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
            answer = response.candidates[0].content.parts[0].text
            processing_time = time.time() - start
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "answer": answer,
                "source": answer_source,
                "confidence": confidence,
                "processing_time": processing_time,
                "use_rag": use_rag or use_hybrid,
                "use_hybrid": use_hybrid,
                "max_similarity": max(source.get('similarity', 0) for source in sources) if sources else 0
            }
            
            print(f"[LLM] ç”Ÿæˆå›ç­”: {answer[:200]} ...")
            print(f"[LLM] å›ç­”æ¥æº: {answer_source}")
            print(f"[LLM] ç½®ä¿¡åº¦: {confidence:.3f}")
            print(f"[LLM] ç”Ÿæˆè€—æ—¶: {processing_time:.2f}ç§’")
            
            return result
        else:
            print("[LLM] LLMå“åº”ä¸­æ²¡æœ‰å¯ç”¨çš„æ–‡æœ¬å†…å®¹")
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›ç­”ã€‚",
                "source": "error",
                "confidence": 0,
                "processing_time": time.time() - start,
                "use_rag": False,
                "use_hybrid": False,
                "max_similarity": 0
            }
    except Exception as e:
        print(f"[LLM] ç”Ÿæˆå›ç­”å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return {
            "answer": "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚",
            "source": "error",
            "confidence": 0,
            "processing_time": time.time() - start,
            "use_rag": False,
            "use_hybrid": False,
            "max_similarity": 0
        }

# ä¿ç•™åŸæœ‰çš„ç®€å•ç‰ˆæœ¬å‡½æ•°ï¼Œç”¨äºå‘åå…¼å®¹
def generate_answer_with_llm_simple(query: str, retrieved_chunks: list[str]) -> str:
    """
    ç®€å•ç‰ˆæœ¬çš„ç”Ÿæˆå‡½æ•°ï¼Œä¿æŒå‘åå…¼å®¹
    """
    result = generate_answer_with_llm(query, retrieved_chunks)
    return result["answer"]

# é…ç½®å®‰å…¨æ—¥å¿—
def setup_security_logging():
    """é…ç½®å®‰å…¨æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('security.log'),
            logging.StreamHandler()
        ]
    )

if __name__ == "__main__":
    setup_security_logging()
    pass 
