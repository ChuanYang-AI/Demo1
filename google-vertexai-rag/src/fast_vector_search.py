"""
é«˜æ•ˆå‘é‡æœç´¢ç³»ç»Ÿ - åŸºäºFAISS + sentence-transformers
æä¾›æ¯«ç§’çº§æ£€ç´¢å’Œå¿«é€Ÿç´¢å¼•æ„å»º
"""

import os
import time
import pickle
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
from sentence_transformers import SentenceTransformer
import faiss
from datetime import datetime

class FastVectorSearch:
    def __init__(self, 
                 model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                 index_file: str = "./cache/faiss_index.bin",
                 metadata_file: str = "./cache/faiss_metadata.json"):
        """
        åˆå§‹åŒ–é«˜æ•ˆå‘é‡æœç´¢ç³»ç»Ÿ
        
        Args:
            model_name: sentence-transformersæ¨¡å‹åç§°
            index_file: FAISSç´¢å¼•æ–‡ä»¶è·¯å¾„
            metadata_file: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.model_name = model_name
        self.index_file = index_file
        self.metadata_file = metadata_file
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(index_file), exist_ok=True)
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        print(f"ğŸ”§ åˆå§‹åŒ–embeddingæ¨¡å‹: {model_name}")
        start_time = time.time()
        self.model = SentenceTransformer(model_name)
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {time.time() - start_time:.2f}ç§’)")
        
        # è·å–embeddingç»´åº¦
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        print(f"ğŸ“ Embeddingç»´åº¦: {self.embedding_dim}")
        
        # åˆå§‹åŒ–FAISSç´¢å¼•
        self.index = None
        self.metadata = {}  # id -> {text, source, timestamp}
        
        # åŠ è½½ç°æœ‰ç´¢å¼•
        self.load_index()
        
    def load_index(self):
        """åŠ è½½ç°æœ‰çš„FAISSç´¢å¼•å’Œå…ƒæ•°æ®"""
        try:
            if os.path.exists(self.index_file) and os.path.exists(self.metadata_file):
                print("ğŸ“‚ åŠ è½½ç°æœ‰ç´¢å¼•...")
                # åŠ è½½FAISSç´¢å¼•
                self.index = faiss.read_index(self.index_file)
                
                # åŠ è½½å…ƒæ•°æ®
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                
                print(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆ - åŒ…å« {self.index.ntotal} ä¸ªå‘é‡")
            else:
                print("ğŸ†• åˆ›å»ºæ–°çš„FAISSç´¢å¼•...")
                # ä½¿ç”¨ç®€å•çš„å¹³é¢ç´¢å¼•ï¼Œé¿å…èšç±»é—®é¢˜
                self.index = faiss.IndexFlatIP(self.embedding_dim)
                
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            print("ğŸ†• åˆ›å»ºæ–°çš„FAISSç´¢å¼•...")
            self.index = faiss.IndexFlatIP(self.embedding_dim)
            self.metadata = {}
    
    def save_index(self):
        """ä¿å­˜FAISSç´¢å¼•å’Œå…ƒæ•°æ®"""
        try:
            # ä¿å­˜FAISSç´¢å¼•
            faiss.write_index(self.index, self.index_file)
            
            # ä¿å­˜å…ƒæ•°æ®
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ç´¢å¼•ä¿å­˜å®Œæˆ - {self.index.ntotal} ä¸ªå‘é‡")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
    
    def generate_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        ç”Ÿæˆæ–‡æœ¬çš„embeddingå‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º (len(texts), embedding_dim)
        """
        print(f"ğŸ”„ ç”Ÿæˆ {len(texts)} ä¸ªæ–‡æœ¬çš„embedding...")
        start_time = time.time()
        
        # æ‰¹é‡ç”Ÿæˆembedding
        embeddings = self.model.encode(texts, 
                                     batch_size=32,
                                     show_progress_bar=False,
                                     convert_to_numpy=True)
        
        # æ ‡å‡†åŒ–å‘é‡ï¼ˆé‡è¦ï¼šç”¨äºå†…ç§¯æœç´¢ï¼‰
        faiss.normalize_L2(embeddings)
        
        print(f"âœ… Embeddingç”Ÿæˆå®Œæˆ (è€—æ—¶: {time.time() - start_time:.2f}ç§’)")
        return embeddings
    
    def add_documents(self, documents: List[Dict[str, str]]) -> List[int]:
        """
        æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å« {id, text, source}
            
        Returns:
            æ·»åŠ çš„æ–‡æ¡£IDåˆ—è¡¨
        """
        if not documents:
            return []
        
        print(f"ğŸ“ æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°ç´¢å¼•...")
        start_time = time.time()
        
        # æå–æ–‡æœ¬
        texts = [doc['text'] for doc in documents]
        
        # ç”Ÿæˆembedding
        embeddings = self.generate_embeddings(texts)
        
        # æ™ºèƒ½ç´¢å¼•ç®¡ç†ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦å‡çº§ç´¢å¼•ç±»å‹
        current_total = self.index.ntotal
        new_total = current_total + len(embeddings)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‡çº§ç´¢å¼•
        if new_total >= 500 and isinstance(self.index, faiss.IndexFlat):
            print("ğŸ“ˆ æ•°æ®é‡è¾¾åˆ°é˜ˆå€¼ï¼Œå‡çº§åˆ°IVFç´¢å¼•...")
            self._upgrade_to_ivf_index()
        
        # è®­ç»ƒç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not self.index.is_trained:
            print("ğŸ‹ï¸ è®­ç»ƒFAISSç´¢å¼•...")
            try:
                self.index.train(embeddings)
            except Exception as e:
                print(f"âš ï¸ ç´¢å¼•è®­ç»ƒå¤±è´¥: {e}")
                # å¦‚æœæ˜¯IVFç´¢å¼•è®­ç»ƒå¤±è´¥ï¼Œé™çº§åˆ°å¹³é¢ç´¢å¼•
                if not isinstance(self.index, faiss.IndexFlat):
                    print("ğŸ”„ é™çº§åˆ°å¹³é¢ç´¢å¼•...")
                    self._downgrade_to_flat_index()
        
        # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        start_idx = self.index.ntotal
        self.index.add(embeddings)
        
        # æ›´æ–°å…ƒæ•°æ®
        added_ids = []
        for i, doc in enumerate(documents):
            doc_id = start_idx + i
            self.metadata[str(doc_id)] = {
                'text': doc['text'],
                'source': doc.get('source', 'unknown'),
                'original_id': doc.get('id', f'doc_{doc_id}'),
                'timestamp': datetime.now().isoformat()
            }
            added_ids.append(doc_id)
        
        # ä¿å­˜ç´¢å¼•
        self.save_index()
        
        print(f"âœ… æ–‡æ¡£æ·»åŠ å®Œæˆ (è€—æ—¶: {time.time() - start_time:.2f}ç§’)")
        print(f"ğŸ“Š ç´¢å¼•æ€»æ•°: {self.index.ntotal}")
        
        return added_ids
    
    def search(self, query: str, k: int = 5) -> List[Dict]:
        """
        æœç´¢æœ€ç›¸å…³çš„æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›çš„ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if self.index.ntotal == 0:
            print("âš ï¸ ç´¢å¼•ä¸ºç©ºï¼Œæ— æ³•æœç´¢")
            return []
        
        print(f"ğŸ” æœç´¢æŸ¥è¯¢: '{query[:50]}...'")
        start_time = time.time()
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.generate_embeddings([query])
        
        # æœç´¢
        scores, indices = self.index.search(query_embedding, k)
        
        # æ„å»ºç»“æœ
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx == -1:  # FAISSæ‰¾ä¸åˆ°è¶³å¤Ÿçš„ç»“æœæ—¶è¿”å›-1
                break
                
            metadata = self.metadata.get(str(idx), {})
            result = {
                'rank': i + 1,
                'score': float(score),
                'similarity': float(score),  # å†…ç§¯æœç´¢ï¼Œåˆ†æ•°å³ä¸ºç›¸ä¼¼åº¦
                'index': int(idx),
                'id': metadata.get('original_id', f'doc_{idx}'),
                'text': metadata.get('text', ''),
                'source': metadata.get('source', 'unknown'),
                'timestamp': metadata.get('timestamp', '')
            }
            results.append(result)
        
        search_time = time.time() - start_time
        print(f"âœ… æœç´¢å®Œæˆ (è€—æ—¶: {search_time*1000:.1f}ms)")
        print(f"ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        
        return results
    
    def get_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_documents': self.index.ntotal if self.index else 0,
            'embedding_dimension': self.embedding_dim,
            'model_name': self.model_name,
            'index_trained': self.index.is_trained if self.index else False,
            'metadata_count': len(self.metadata)
        }
    
    def clear_index(self):
        """æ¸…ç©ºç´¢å¼•"""
        print("ğŸ—‘ï¸ æ¸…ç©ºç´¢å¼•...")
        self.index.reset()
        self.metadata.clear()
        self.save_index()
        print("âœ… ç´¢å¼•å·²æ¸…ç©º")
    
    def _upgrade_to_ivf_index(self):
        """å‡çº§åˆ°IVFç´¢å¼•ä»¥æé«˜å¤§æ•°æ®é›†çš„æœç´¢æ€§èƒ½"""
        try:
            # è·å–å½“å‰æ‰€æœ‰å‘é‡
            if self.index.ntotal == 0:
                return
            
            # é‡æ„æ‰€æœ‰å‘é‡
            all_vectors = np.zeros((self.index.ntotal, self.embedding_dim), dtype='float32')
            for i in range(self.index.ntotal):
                all_vectors[i] = self.index.reconstruct(i)
            
            # åŠ¨æ€è®¡ç®—èšç±»æ•°é‡ï¼šå»ºè®®æ˜¯æ•°æ®ç‚¹æ•°é‡çš„å¹³æ–¹æ ¹ï¼Œæœ€å°‘10ä¸ªï¼Œæœ€å¤š256ä¸ª
            n_clusters = min(max(int(np.sqrt(self.index.ntotal)), 10), 256)
            
            # åˆ›å»ºæ–°çš„IVFç´¢å¼•
            quantizer = faiss.IndexFlatIP(self.embedding_dim)
            new_index = faiss.IndexIVFFlat(quantizer, self.embedding_dim, n_clusters)
            
            # è®­ç»ƒæ–°ç´¢å¼•
            new_index.train(all_vectors)
            
            # æ·»åŠ æ‰€æœ‰å‘é‡åˆ°æ–°ç´¢å¼•
            new_index.add(all_vectors)
            
            # æ›¿æ¢æ—§ç´¢å¼•
            self.index = new_index
            print(f"âœ… æˆåŠŸå‡çº§åˆ°IVFç´¢å¼• (èšç±»æ•°: {n_clusters})")
            
        except Exception as e:
            print(f"âŒ å‡çº§ç´¢å¼•å¤±è´¥: {e}")
            print("ğŸ”„ ä¿æŒä½¿ç”¨å¹³é¢ç´¢å¼•...")
    
    def _downgrade_to_flat_index(self):
        """é™çº§åˆ°å¹³é¢ç´¢å¼•ä»¥é¿å…è®­ç»ƒé—®é¢˜"""
        try:
            # è·å–å½“å‰æ‰€æœ‰å‘é‡
            if self.index.ntotal == 0:
                # åˆ›å»ºç©ºçš„å¹³é¢ç´¢å¼•
                self.index = faiss.IndexFlatIP(self.embedding_dim)
                return
            
            # é‡æ„æ‰€æœ‰å‘é‡
            all_vectors = np.zeros((self.index.ntotal, self.embedding_dim), dtype='float32')
            for i in range(self.index.ntotal):
                all_vectors[i] = self.index.reconstruct(i)
            
            # åˆ›å»ºæ–°çš„å¹³é¢ç´¢å¼•
            new_index = faiss.IndexFlatIP(self.embedding_dim)
            
            # æ·»åŠ æ‰€æœ‰å‘é‡åˆ°æ–°ç´¢å¼•
            new_index.add(all_vectors)
            
            # æ›¿æ¢æ—§ç´¢å¼•
            self.index = new_index
            print("âœ… æˆåŠŸé™çº§åˆ°å¹³é¢ç´¢å¼•")
            
        except Exception as e:
            print(f"âŒ é™çº§ç´¢å¼•å¤±è´¥: {e}")
            # åˆ›å»ºå…¨æ–°çš„å¹³é¢ç´¢å¼•
            self.index = faiss.IndexFlatIP(self.embedding_dim)
            print("ğŸ†• åˆ›å»ºæ–°çš„å¹³é¢ç´¢å¼•")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå‘é‡æœç´¢å®ä¾‹
    search_engine = FastVectorSearch()
    
    # ç¤ºä¾‹æ–‡æ¡£
    documents = [
        {"id": "doc1", "text": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯", "source": "AIæ•™ç¨‹"},
        {"id": "doc2", "text": "æœºå™¨å­¦ä¹ æ˜¯å®ç°äººå·¥æ™ºèƒ½çš„é‡è¦æ–¹æ³•", "source": "MLæŒ‡å—"},
        {"id": "doc3", "text": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†", "source": "DLæ¦‚è¿°"},
        {"id": "doc4", "text": "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯AIçš„é‡è¦åº”ç”¨é¢†åŸŸ", "source": "NLPç®€ä»‹"},
        {"id": "doc5", "text": "è®¡ç®—æœºè§†è§‰å¸®åŠ©æœºå™¨ç†è§£å›¾åƒ", "source": "CVåŸºç¡€"}
    ]
    
    # æ·»åŠ æ–‡æ¡£
    doc_ids = search_engine.add_documents(documents)
    print(f"æ·»åŠ çš„æ–‡æ¡£ID: {doc_ids}")
    
    # æœç´¢æµ‹è¯•
    results = search_engine.search("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½", k=3)
    for result in results:
        print(f"æ’å{result['rank']}: {result['text'][:50]}... (ç›¸ä¼¼åº¦: {result['similarity']:.3f})")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = search_engine.get_stats()
    print(f"ç´¢å¼•ç»Ÿè®¡: {stats}") 