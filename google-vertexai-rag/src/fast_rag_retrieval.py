"""
é«˜æ•ˆRAGæ£€ç´¢æ¨¡å— - é›†æˆFAISSå‘é‡æœç´¢
æä¾›æ¯«ç§’çº§æ£€ç´¢æ€§èƒ½å’Œå¿«é€Ÿç´¢å¼•æ„å»º
"""

import time
import os
from typing import List, Dict, Optional
try:
    from .fast_vector_search import FastVectorSearch
    from .data_preprocessing import chunk_text
except ImportError:
    from fast_vector_search import FastVectorSearch
    from data_preprocessing import chunk_text

class FastRAGRetrieval:
    def __init__(self, 
                 model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                 cache_dir: str = "./cache"):
        """
        åˆå§‹åŒ–é«˜æ•ˆRAGæ£€ç´¢ç³»ç»Ÿ
        
        Args:
            model_name: sentence-transformersæ¨¡å‹åç§°
            cache_dir: ç¼“å­˜ç›®å½•
        """
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
        index_file = os.path.join(cache_dir, "faiss_index.bin")
        metadata_file = os.path.join(cache_dir, "faiss_metadata.json")
        
        print("ğŸš€ åˆå§‹åŒ–é«˜æ•ˆå‘é‡æœç´¢å¼•æ“...")
        start_time = time.time()
        
        self.search_engine = FastVectorSearch(
            model_name=model_name,
            index_file=index_file,
            metadata_file=metadata_file
        )
        
        print(f"âœ… é«˜æ•ˆæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {time.time() - start_time:.2f}ç§’)")
        
        # æ˜¾ç¤ºåˆå§‹ç»Ÿè®¡ä¿¡æ¯
        stats = self.search_engine.get_stats()
        print(f"ğŸ“Š å½“å‰ç´¢å¼•çŠ¶æ€: {stats['total_documents']} ä¸ªæ–‡æ¡£, {stats['embedding_dimension']} ç»´å‘é‡")
    
    def add_document(self, file_id: str, text: str, filename: str = None) -> int:
        """
        æ·»åŠ å•ä¸ªæ–‡æ¡£åˆ°ç´¢å¼•
        
        Args:
            file_id: æ–‡ä»¶ID
            text: æ–‡æ¡£æ–‡æœ¬
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ·»åŠ çš„æ–‡æ¡£å—æ•°é‡
        """
        print(f"ğŸ“ å¤„ç†æ–‡æ¡£: {filename or file_id}")
        
        # åˆ†å—å¤„ç†
        chunks = chunk_text(text, chunk_size=500, overlap_size=100)
        
        if not chunks:
            print("âš ï¸ æ–‡æ¡£ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
            return 0
        
        # å‡†å¤‡æ–‡æ¡£æ•°æ®
        documents = []
        for i, chunk in enumerate(chunks):
            doc = {
                'id': f"{file_id}_chunk_{i}",
                'text': chunk,
                'source': filename or file_id
            }
            documents.append(doc)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        added_ids = self.search_engine.add_documents(documents)
        
        print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {len(added_ids)} ä¸ªå—å·²æ·»åŠ åˆ°ç´¢å¼•")
        return len(added_ids)
    
    def add_documents_batch(self, documents: List[Dict[str, str]]) -> int:
        """
        æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ [{file_id, text, filename}, ...]
            
        Returns:
            æ·»åŠ çš„æ–‡æ¡£å—æ€»æ•°
        """
        if not documents:
            return 0
        
        print(f"ğŸ“š æ‰¹é‡å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£...")
        start_time = time.time()
        
        all_chunks = []
        chunk_count = 0
        
        for doc in documents:
            file_id = doc['file_id']
            text = doc['text']
            filename = doc.get('filename', file_id)
            
            # åˆ†å—å¤„ç†
            chunks = chunk_text(text, chunk_size=500, overlap_size=100)
            
            # å‡†å¤‡æ–‡æ¡£æ•°æ®
            for i, chunk in enumerate(chunks):
                chunk_doc = {
                    'id': f"{file_id}_chunk_{i}",
                    'text': chunk,
                    'source': filename
                }
                all_chunks.append(chunk_doc)
                chunk_count += 1
        
        # æ‰¹é‡æ·»åŠ åˆ°ç´¢å¼•
        if all_chunks:
            added_ids = self.search_engine.add_documents(all_chunks)
            
            print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ (è€—æ—¶: {time.time() - start_time:.2f}ç§’)")
            print(f"ğŸ“Š æ€»å…±æ·»åŠ  {len(added_ids)} ä¸ªæ–‡æ¡£å—")
            return len(added_ids)
        
        return 0
    
    def search(self, query: str, k: int = 5, min_score: float = 0.3) -> List[Dict]:
        """
        æœç´¢æœ€ç›¸å…³çš„æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›çš„ç»“æœæ•°é‡
            min_score: æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        print(f"ğŸ” æ‰§è¡Œé«˜æ•ˆæœç´¢: '{query[:50]}...'")
        start_time = time.time()
        
        # æ‰§è¡Œæœç´¢
        results = self.search_engine.search(query, k=k)
        
        # è¿‡æ»¤ä½åˆ†ç»“æœ
        filtered_results = []
        for result in results:
            if result['similarity'] >= min_score:
                # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
                compatible_result = {
                    'id': result['id'],
                    'datapoint_id': result['id'],
                    'distance': 1.0 - result['similarity'],  # è½¬æ¢ä¸ºè·ç¦»
                    'similarity': result['similarity'],
                    'score': result['score'],
                    'rank': result['rank'],
                    'text': result['text'],
                    'source': result['source'],
                    'content_preview': result['text'][:100] + '...' if len(result['text']) > 100 else result['text']
                }
                filtered_results.append(compatible_result)
        
        search_time = time.time() - start_time
        print(f"âœ… é«˜æ•ˆæœç´¢å®Œæˆ (è€—æ—¶: {search_time*1000:.1f}ms)")
        print(f"ğŸ“‹ æ‰¾åˆ° {len(filtered_results)} ä¸ªç›¸å…³ç»“æœ")
        
        # æ˜¾ç¤ºæœç´¢ç»“æœæ‘˜è¦
        for i, result in enumerate(filtered_results[:3]):
            print(f"   {i+1}. {result['source']}: {result['text'][:50]}... (ç›¸ä¼¼åº¦: {result['similarity']:.3f})")
        
        return filtered_results
    
    def get_stats(self) -> Dict:
        """è·å–æ£€ç´¢ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        base_stats = self.search_engine.get_stats()
        return {
            'retrieval_engine': 'FastRAG + FAISS',
            'total_documents': base_stats['total_documents'],
            'embedding_dimension': base_stats['embedding_dimension'],
            'model_name': base_stats['model_name'],
            'index_trained': base_stats['index_trained'],
            'cache_dir': self.cache_dir,
            'performance': 'æ¯«ç§’çº§æ£€ç´¢'
        }
    
    def clear_index(self):
        """æ¸…ç©ºç´¢å¼•"""
        print("ğŸ—‘ï¸ æ¸…ç©ºæ£€ç´¢ç´¢å¼•...")
        self.search_engine.clear_index()
        print("âœ… ç´¢å¼•å·²æ¸…ç©º")
    
    def rebuild_index(self, documents: List[Dict[str, str]]):
        """é‡å»ºç´¢å¼•"""
        print("ğŸ”„ é‡å»ºæ£€ç´¢ç´¢å¼•...")
        start_time = time.time()
        
        # æ¸…ç©ºç°æœ‰ç´¢å¼•
        self.clear_index()
        
        # é‡æ–°æ·»åŠ æ–‡æ¡£
        added_count = self.add_documents_batch(documents)
        
        print(f"âœ… ç´¢å¼•é‡å»ºå®Œæˆ (è€—æ—¶: {time.time() - start_time:.2f}ç§’)")
        print(f"ğŸ“Š é‡å»ºç»“æœ: {added_count} ä¸ªæ–‡æ¡£å—")
        
        return added_count

# ä¾¿æ·å‡½æ•°ï¼šæ›¿æ¢åŸæœ‰çš„retrieve_relevant_chunks
def retrieve_relevant_chunks_fast(
    query_text: str,
    retrieval_engine: FastRAGRetrieval,
    num_neighbors: int = 5,
    min_similarity: float = 0.3,
    **kwargs  # å…¼å®¹æ—§å‚æ•°
) -> List[Dict]:
    """
    é«˜æ•ˆæ£€ç´¢ç›¸å…³æ–‡æ¡£å— - å…¼å®¹åŸæœ‰æ¥å£
    
    Args:
        query_text: æŸ¥è¯¢æ–‡æœ¬
        retrieval_engine: é«˜æ•ˆæ£€ç´¢å¼•æ“å®ä¾‹
        num_neighbors: è¿”å›çš„é‚»å±…æ•°é‡
        min_similarity: æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
        
    Returns:
        æ£€ç´¢ç»“æœåˆ—è¡¨
    """
    print(f"[FastRAG] ä½¿ç”¨é«˜æ•ˆæ£€ç´¢å¼•æ“è¿›è¡Œæœç´¢...")
    
    # æ‰§è¡Œæœç´¢
    results = retrieval_engine.search(
        query=query_text,
        k=num_neighbors,
        min_score=min_similarity
    )
    
    print(f"[FastRAG] é«˜æ•ˆæ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
    return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºé«˜æ•ˆæ£€ç´¢ç³»ç»Ÿ
    retrieval_system = FastRAGRetrieval()
    
    # ç¤ºä¾‹æ–‡æ¡£
    documents = [
        {
            'file_id': 'legal_doc_1',
            'text': 'åˆåŒæ˜¯å½“äº‹äººä¹‹é—´è®¾ç«‹ã€å˜æ›´ã€ç»ˆæ­¢æ°‘äº‹æƒåˆ©ä¹‰åŠ¡å…³ç³»çš„åè®®ã€‚åˆåŒçš„æˆç«‹éœ€è¦å½“äº‹äººè¾¾æˆåˆæ„ï¼Œå¹¶ä¸”åˆæ„çš„å†…å®¹å¿…é¡»åˆæ³•ã€‚',
            'filename': 'åˆåŒæ³•åŸºç¡€.docx'
        },
        {
            'file_id': 'legal_doc_2', 
            'text': 'æ°‘äº‹è´£ä»»æ˜¯æŒ‡æ°‘äº‹ä¸»ä½“å› è¿åæ°‘äº‹ä¹‰åŠ¡è€Œæ‰¿æ‹…çš„æ³•å¾‹åæœã€‚æ°‘äº‹è´£ä»»çš„æ‰¿æ‹…æ–¹å¼åŒ…æ‹¬åœæ­¢ä¾µå®³ã€èµ”å¿æŸå¤±ã€æ¢å¤åŸçŠ¶ç­‰ã€‚',
            'filename': 'æ°‘äº‹è´£ä»».docx'
        }
    ]
    
    # æ‰¹é‡æ·»åŠ æ–‡æ¡£
    added_count = retrieval_system.add_documents_batch(documents)
    print(f"æ·»åŠ äº† {added_count} ä¸ªæ–‡æ¡£å—")
    
    # æœç´¢æµ‹è¯•
    results = retrieval_system.search("ä»€ä¹ˆæ˜¯åˆåŒ", k=3)
    for result in results:
        print(f"ç›¸ä¼¼åº¦ {result['similarity']:.3f}: {result['text'][:50]}...")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = retrieval_system.get_stats()
    print(f"ç³»ç»Ÿç»Ÿè®¡: {stats}") 