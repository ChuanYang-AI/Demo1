"""
æ··åˆæ£€ç´¢ç³»ç»Ÿ - åŒè·¯å¬å› + æ™ºèƒ½èåˆ
é›†æˆFAISSå¿«é€Ÿæ£€ç´¢å’ŒVertex AIæ£€ç´¢ï¼Œæä¾›æœ€ä½³çš„é€Ÿåº¦å’Œå‡†ç¡®æ€§å¹³è¡¡
"""

import time
import asyncio
import threading
from typing import List, Dict, Optional, Tuple, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from enum import Enum
import numpy as np

# å¯¼å…¥ç°æœ‰æ¨¡å—
try:
    from .fast_rag_retrieval import FastRAGRetrieval
    from .rag_retrieval import retrieve_relevant_chunks
    from .rag_generation import generate_answer_with_llm
except ImportError:
    # å¤„ç†ç»å¯¹å¯¼å…¥æƒ…å†µ
    from fast_rag_retrieval import FastRAGRetrieval
    from rag_retrieval import retrieve_relevant_chunks
    from rag_generation import generate_answer_with_llm

class RetrievalStrategy(Enum):
    """æ£€ç´¢ç­–ç•¥æšä¸¾"""
    FAST_ONLY = "fast_only"           # ä»…ä½¿ç”¨FAISSå¿«é€Ÿæ£€ç´¢
    VERTEX_ONLY = "vertex_only"       # ä»…ä½¿ç”¨Vertex AIæ£€ç´¢
    HYBRID_PARALLEL = "hybrid_parallel"  # å¹¶è¡Œæ··åˆæ£€ç´¢
    ADAPTIVE = "adaptive"             # è‡ªé€‚åº”æ£€ç´¢
    FALLBACK = "fallback"            # é™çº§æ£€ç´¢

@dataclass
class RetrievalConfig:
    """æ£€ç´¢é…ç½®"""
    # åŸºç¡€é…ç½®
    num_candidates: int = 10          # æ¯è·¯å¬å›çš„å€™é€‰æ•°
    final_results: int = 5            # æœ€ç»ˆè¿”å›ç»“æœæ•°
    
    # ç³»ç»Ÿæƒé‡
    faiss_weight: float = 0.6         # FAISSç³»ç»Ÿæƒé‡
    vertex_weight: float = 0.4        # Vertex AIç³»ç»Ÿæƒé‡
    
    # é˜ˆå€¼é…ç½®
    min_similarity: float = 0.3       # æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
    high_confidence_threshold: float = 0.8  # é«˜ç½®ä¿¡åº¦é˜ˆå€¼
    
    # æ€§èƒ½é…ç½®
    max_parallel_timeout: float = 5.0  # å¹¶è¡Œæ£€ç´¢è¶…æ—¶æ—¶é—´
    fallback_threshold: float = 2.0    # é™çº§æ£€ç´¢é˜ˆå€¼(ç§’)
    
    # èåˆç®—æ³•é…ç½®
    rrf_k: int = 60                   # RRFç®—æ³•å‚æ•°
    enable_reranking: bool = True     # æ˜¯å¦å¯ç”¨é‡æ’åº

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    id: str
    text: str
    source: str
    similarity: float
    distance: float
    rank: int
    retrieval_source: str             # 'faiss', 'vertex', 'hybrid'
    confidence: float                 # ç½®ä¿¡åº¦åˆ†æ•°
    metadata: Dict = None

class HybridRetrieval:
    """æ··åˆæ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(self, 
                 config: RetrievalConfig = None,
                 project_id: str = None,
                 location: str = None,
                 endpoint_id: str = None):
        """åˆå§‹åŒ–æ··åˆæ£€ç´¢ç³»ç»Ÿ"""
        self.config = config or RetrievalConfig()
        self.project_id = project_id
        self.location = location  
        self.endpoint_id = endpoint_id
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_queries': 0,
            'fast_success': 0,
            'vertex_success': 0,
            'hybrid_success': 0,
            'fallback_used': 0,
            'total_time': 0.0,  # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
            'avg_response_time': 0.0
        }
        
        # å­˜å‚¨ç»„ä»¶
        self.fast_retrieval = None
        self.chunk_map = {}
        self.chunk_embeddings = {}
        
        # æ·»åŠ çº¿ç¨‹æ± æ‰§è¡Œå™¨
        from concurrent.futures import ThreadPoolExecutor
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        print("ğŸ”§ åˆå§‹åŒ–æ··åˆæ£€ç´¢ç³»ç»Ÿ...")
        self._initialize_components()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–æ£€ç´¢ç»„ä»¶"""
        try:
            # åˆå§‹åŒ–FAISSå¿«é€Ÿæ£€ç´¢
            print("âš¡ åˆå§‹åŒ–FAISSæ£€ç´¢å¼•æ“...")
            self.fast_retrieval = FastRAGRetrieval()
            print("âœ… FAISSæ£€ç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ FAISSæ£€ç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.fast_retrieval = None
    
    def add_document(self, file_id: str, text: str, filename: str = None) -> bool:
        """
        æ·»åŠ æ–‡æ¡£åˆ°æ··åˆç´¢å¼•
        
        Args:
            file_id: æ–‡ä»¶ID
            text: æ–‡æ¡£æ–‡æœ¬
            filename: æ–‡ä»¶å
            
        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        success = True
        
        # æ·»åŠ åˆ°FAISSç´¢å¼•
        if self.fast_retrieval:
            try:
                chunks_added = self.fast_retrieval.add_document(file_id, text, filename)
                print(f"âœ… FAISSç´¢å¼•æ·»åŠ å®Œæˆ: {chunks_added} ä¸ªå—")
            except Exception as e:
                print(f"âŒ FAISSç´¢å¼•æ·»åŠ å¤±è´¥: {e}")
                # å¯¹äºFAISSç´¢å¼•å¤±è´¥ï¼Œä¸å½±å“æ•´ä½“æµç¨‹
                print("âš ï¸ ç»§ç»­ä½¿ç”¨Vertex AIæ£€ç´¢...")
                success = True  # ä¿æŒä¸ºTrueï¼Œå› ä¸ºè¿˜æœ‰å…¶ä»–æ£€ç´¢æ–¹å¼
        
        # æ·»åŠ åˆ°åŸæœ‰ç³»ç»Ÿçš„chunk_map (å…¼å®¹æ€§)
        try:
            from .data_preprocessing import chunk_text
        except ImportError:
            from data_preprocessing import chunk_text
        chunks = chunk_text(text, chunk_size=500, overlap_size=100)
        for i, chunk in enumerate(chunks):
            chunk_id = f"file_{file_id}_chunk_{i}"
            self.chunk_map[chunk_id] = chunk
        
        return success
    
    def search(self, 
               query: str, 
               strategy: RetrievalStrategy = RetrievalStrategy.HYBRID_PARALLEL,
               **kwargs) -> List[RetrievalResult]:
        """
        æ‰§è¡Œæ··åˆæ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            strategy: æ£€ç´¢ç­–ç•¥
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            List[RetrievalResult]: æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # ä¿å­˜æŸ¥è¯¢ç”¨äºå…³é”®è¯åŒ¹é…
        self.last_query = query
        
        self.stats['total_queries'] += 1
        print(f"ğŸ” å¼€å§‹æ··åˆæ£€ç´¢: {query}")
        print(f"ğŸ“Š ç­–ç•¥: {strategy.value}")
        
        start_time = time.time()
        
        try:
            # æ ¹æ®ç­–ç•¥é€‰æ‹©æ£€ç´¢æ–¹æ³•
            if strategy == RetrievalStrategy.FAST_ONLY:
                results = self._search_fast_only(query)
            elif strategy == RetrievalStrategy.VERTEX_ONLY:
                results = self._search_vertex_only(query)
            elif strategy == RetrievalStrategy.HYBRID_PARALLEL:
                results = self._search_hybrid_parallel(query)
            elif strategy == RetrievalStrategy.ADAPTIVE:
                results = self._search_adaptive(query)
            elif strategy == RetrievalStrategy.FALLBACK:
                results = self._search_fallback(query)
            else:
                print(f"âš ï¸ æœªçŸ¥æ£€ç´¢ç­–ç•¥: {strategy}")
                results = self._search_hybrid_parallel(query)
            
            # ç»Ÿè®¡æ€§èƒ½
            processing_time = time.time() - start_time
            self.stats['total_time'] += processing_time
            
            if results:
                if strategy == RetrievalStrategy.FAST_ONLY:
                    self.stats['fast_success'] += 1
                elif strategy == RetrievalStrategy.VERTEX_ONLY:
                    self.stats['vertex_success'] += 1
                else:
                    self.stats['hybrid_success'] += 1
                
                print(f"âœ… æ··åˆæ£€ç´¢æˆåŠŸ: {len(results)} ä¸ªç»“æœ, è€—æ—¶ {processing_time:.2f}s")
                
                # æ‰“å°å‰3ä¸ªç»“æœçš„ç›¸å…³ä¿¡æ¯
                for i, result in enumerate(results[:3]):
                    print(f"  ğŸ“„ #{i+1}: {result.id} (ç›¸ä¼¼åº¦: {result.similarity:.3f}, ç½®ä¿¡åº¦: {result.confidence:.3f})")
            else:
                print(f"âš ï¸ æ··åˆæ£€ç´¢æ— ç»“æœ, è€—æ—¶ {processing_time:.2f}s")
            
            return results
            
        except Exception as e:
            print(f"âŒ æ··åˆæ£€ç´¢å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _search_fast_only(self, query: str) -> List[RetrievalResult]:
        """ä»…ä½¿ç”¨FAISSå¿«é€Ÿæ£€ç´¢"""
        if not self.fast_retrieval:
            raise Exception("FAISSæ£€ç´¢ç³»ç»Ÿä¸å¯ç”¨")
        
        results = self.fast_retrieval.search(
            query, 
            k=self.config.final_results,
            min_score=self.config.min_similarity
        )
        
        self.stats['fast_success'] += 1
        return self._convert_to_retrieval_results(results, 'faiss')
    
    def _search_vertex_only(self, query: str) -> List[RetrievalResult]:
        """ä»…ä½¿ç”¨Vertex AIæ£€ç´¢"""
        results = retrieve_relevant_chunks(
            project_id=self.project_id,
            location=self.location,
            endpoint_id=self.endpoint_id,
            query_text=query,
            num_neighbors=self.config.final_results,
            chunk_map=self.chunk_map,
            chunk_embeddings=self.chunk_embeddings
        )
        
        self.stats['vertex_success'] += 1
        return self._convert_to_retrieval_results(results, 'vertex')
    
    def _search_hybrid_parallel(self, query: str) -> List[RetrievalResult]:
        """å¹¶è¡Œæ··åˆæ£€ç´¢"""
        print("ğŸ”„ å¯åŠ¨å¹¶è¡ŒåŒè·¯å¬å›...")
        
        # å¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªæ£€ç´¢ç³»ç»Ÿ
        futures = {}
        
        # æäº¤FAISSæ£€ç´¢ä»»åŠ¡
        if self.fast_retrieval:
            futures['faiss'] = self.executor.submit(
                self._safe_search_faiss, query
            )
        
        # æäº¤Vertex AIæ£€ç´¢ä»»åŠ¡
        futures['vertex'] = self.executor.submit(
            self._safe_search_vertex, query
        )
        
        # æ”¶é›†ç»“æœ
        faiss_results = []
        vertex_results = []
        
        for name, future in futures.items():
            try:
                result = future.result(timeout=self.config.max_parallel_timeout)
                if name == 'faiss' and result:
                    faiss_results = result
                    print(f"âœ… FAISSæ£€ç´¢å®Œæˆ: {len(result)} ä¸ªç»“æœ")
                elif name == 'vertex' and result:
                    vertex_results = result
                    print(f"âœ… Vertexæ£€ç´¢å®Œæˆ: {len(result)} ä¸ªç»“æœ")
            except Exception as e:
                print(f"âš ï¸ {name}æ£€ç´¢å¤±è´¥: {e}")
        
        # èåˆç»“æœ
        if faiss_results or vertex_results:
            merged_results = self._merge_results(faiss_results, vertex_results)
            self.stats['hybrid_success'] += 1
            return merged_results
        else:
            raise Exception("æ‰€æœ‰æ£€ç´¢è·¯å¾„éƒ½å¤±è´¥")
    
    def _search_adaptive(self, query: str) -> List[RetrievalResult]:
        """è‡ªé€‚åº”æ£€ç´¢ - æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦åŠ¨æ€é€‰æ‹©ç­–ç•¥"""
        # ç®€å•çš„è‡ªé€‚åº”é€»è¾‘ï¼šçŸ­æŸ¥è¯¢ä½¿ç”¨å¿«é€Ÿæ£€ç´¢ï¼Œé•¿æŸ¥è¯¢ä½¿ç”¨æ··åˆæ£€ç´¢
        if len(query) < 10:
            print("ğŸ“ çŸ­æŸ¥è¯¢ï¼Œä½¿ç”¨å¿«é€Ÿæ£€ç´¢")
            return self._search_fast_only(query)
        else:
            print("ğŸ“ å¤æ‚æŸ¥è¯¢ï¼Œä½¿ç”¨æ··åˆæ£€ç´¢")
            return self._search_hybrid_parallel(query)
    
    def _search_fallback(self, query: str) -> List[RetrievalResult]:
        """é™çº§æ£€ç´¢ - æŒ‰ä¼˜å…ˆçº§å°è¯•å¯ç”¨ç³»ç»Ÿ"""
        print("ğŸ†˜ å¯åŠ¨é™çº§æ£€ç´¢æ¨¡å¼...")
        
        # ä¼˜å…ˆå°è¯•FAISS (é€Ÿåº¦å¿«)
        if self.fast_retrieval:
            try:
                results = self._safe_search_faiss(query)
                if results:
                    print("âœ… é™çº§åˆ°FAISSæ£€ç´¢æˆåŠŸ")
                    self.stats['fallback_used'] += 1
                    return results
            except Exception as e:
                print(f"âš ï¸ FAISSé™çº§å¤±è´¥: {e}")
        
        # å°è¯•Vertex AI
        try:
            results = self._safe_search_vertex(query)
            if results:
                print("âœ… é™çº§åˆ°Vertex AIæ£€ç´¢æˆåŠŸ")
                self.stats['fallback_used'] += 1
                return results
        except Exception as e:
            print(f"âš ï¸ Vertex AIé™çº§å¤±è´¥: {e}")
        
        print("âŒ æ‰€æœ‰æ£€ç´¢è·¯å¾„éƒ½ä¸å¯ç”¨")
        return []
    
    def _safe_search_faiss(self, query: str) -> List[RetrievalResult]:
        """å®‰å…¨çš„FAISSæ£€ç´¢"""
        if not self.fast_retrieval:
            return []
        
        try:
            results = self.fast_retrieval.search(
                query,
                k=self.config.num_candidates,
                min_score=self.config.min_similarity
            )
            return self._convert_to_retrieval_results(results, 'faiss')
        except Exception as e:
            print(f"FAISSæ£€ç´¢å¼‚å¸¸: {e}")
            return []
    
    def _safe_search_vertex(self, query: str) -> List[RetrievalResult]:
        """å®‰å…¨çš„Vertex AIæ£€ç´¢"""
        try:
            results = retrieve_relevant_chunks(
                project_id=self.project_id,
                location=self.location,
                endpoint_id=self.endpoint_id,
                query_text=query,
                num_neighbors=self.config.num_candidates,
                chunk_map=self.chunk_map,
                chunk_embeddings=self.chunk_embeddings
            )
            return self._convert_to_retrieval_results(results, 'vertex')
        except Exception as e:
            print(f"Vertex AIæ£€ç´¢å¼‚å¸¸: {e}")
            return []
    
    def _merge_results(self, 
                      faiss_results: List[RetrievalResult], 
                      vertex_results: List[RetrievalResult]) -> List[RetrievalResult]:
        """èåˆä¸¤è·¯æ£€ç´¢ç»“æœ"""
        print("ğŸ”„ å¼€å§‹ç»“æœèåˆ...")
        
        # 1. å»é‡ - åŸºäºæ–‡æœ¬å†…å®¹çš„ç›¸ä¼¼åº¦
        unique_results = self._deduplicate_results(faiss_results + vertex_results)
        
        # 2. é‡æ–°æ’åº - ä½¿ç”¨RRFç®—æ³•
        if self.config.enable_reranking:
            ranked_results = self._reciprocal_rank_fusion(
                faiss_results, vertex_results, unique_results
            )
        else:
            # ç®€å•åŠ æƒèåˆ
            ranked_results = self._weighted_fusion(unique_results)
        
        # 3. è¿”å›Top-Kç»“æœ
        final_results = ranked_results[:self.config.final_results]
        
        print(f"ğŸ”„ ç»“æœèåˆå®Œæˆ: {len(faiss_results)} + {len(vertex_results)} â†’ {len(final_results)}")
        return final_results
    
    def _deduplicate_results(self, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """ç»“æœå»é‡"""
        seen_ids = set()
        unique_results = []
        
        for result in results:
            if result.id not in seen_ids:
                seen_ids.add(result.id)
                unique_results.append(result)
        
        return unique_results
    
    def _reciprocal_rank_fusion(self, 
                               faiss_results: List[RetrievalResult],
                               vertex_results: List[RetrievalResult],
                               all_results: List[RetrievalResult]) -> List[RetrievalResult]:
        """å€’æ•°æ’åºèåˆç®—æ³•ï¼Œå¢åŠ å…³é”®è¯åŒ¹é…æƒé‡"""
        scores = {}
        
        # ä¸ºæ¯ä¸ªç»“æœåˆ›å»ºIDåˆ°ç»“æœçš„æ˜ å°„
        result_map = {r.id: r for r in all_results}
        
        # æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆç®€å•åˆ†è¯ï¼‰
        query_keywords = set(self.last_query.lower().split()) if hasattr(self, 'last_query') else set()
        
        # FAISSç»“æœè¯„åˆ†
        for rank, result in enumerate(faiss_results):
            rrf_score = 1.0 / (self.config.rrf_k + rank + 1)
            
            # å…³é”®è¯åŒ¹é…åŠ æƒ
            keyword_boost = self._calculate_keyword_boost(result.text, query_keywords)
            
            # ç›¸ä¼¼åº¦åŠ æƒ
            similarity_boost = result.similarity * 0.3  # ç»™ç›¸ä¼¼åº¦ä¸€å®šæƒé‡
            
            total_score = rrf_score * self.config.faiss_weight + keyword_boost + similarity_boost
            scores[result.id] = scores.get(result.id, 0) + total_score
        
        # Vertex AIç»“æœè¯„åˆ†
        for rank, result in enumerate(vertex_results):
            rrf_score = 1.0 / (self.config.rrf_k + rank + 1)
            
            # å…³é”®è¯åŒ¹é…åŠ æƒ
            keyword_boost = self._calculate_keyword_boost(result.text, query_keywords)
            
            # ç›¸ä¼¼åº¦åŠ æƒ
            similarity_boost = result.similarity * 0.3
            
            total_score = rrf_score * self.config.vertex_weight + keyword_boost + similarity_boost
            scores[result.id] = scores.get(result.id, 0) + total_score
        
        # æ’åºå¹¶æ·»åŠ èåˆåçš„ç½®ä¿¡åº¦
        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        ranked_results = []
        for rank, (result_id, score) in enumerate(sorted_items):
            if result_id in result_map:
                result = result_map[result_id]
                result.rank = rank + 1
                result.confidence = min(score * 0.5, 1.0)  # æ ‡å‡†åŒ–ç½®ä¿¡åº¦
                result.retrieval_source = 'hybrid'
                ranked_results.append(result)
        
        return ranked_results
    
    def _calculate_keyword_boost(self, text: str, query_keywords: set) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…æƒé‡ï¼Œå¢å¼ºç›¸å…³æ€§åˆ¤æ–­"""
        if not query_keywords or not text:
            return 0.0
        
        text_lower = text.lower()
        matches = 0
        exact_matches = 0
        
        for keyword in query_keywords:
            if keyword in text_lower:
                matches += 1
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç²¾ç¡®åŒ¹é…ï¼ˆå‘¨å›´æœ‰ç©ºæ ¼æˆ–æ ‡ç‚¹ï¼‰
                import re
                if re.search(rf'\b{re.escape(keyword)}\b', text_lower):
                    exact_matches += 1
        
        # è®¡ç®—åŒ¹é…åº¦
        match_ratio = matches / len(query_keywords) if query_keywords else 0.0
        exact_ratio = exact_matches / len(query_keywords) if query_keywords else 0.0
        
        # ç»™ç²¾ç¡®åŒ¹é…æ›´é«˜æƒé‡ï¼Œç»™å…³é”®è¯åŒ¹é…å¾ˆé«˜çš„æƒé‡
        keyword_boost = (match_ratio * 0.6) + (exact_ratio * 0.8)
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœåŒ…å«æŸ¥è¯¢çš„æ ¸å¿ƒæ¦‚å¿µï¼Œç»™é¢å¤–åŠ åˆ†
        core_concepts = {'å®šé‡‘', 'è®¢é‡‘', 'åŒºåˆ«', 'å·®åˆ«', 'ä¸åŒ'}
        concept_matches = sum(1 for concept in core_concepts if concept in text_lower)
        concept_boost = min(concept_matches * 0.3, 0.9)  # æœ€å¤šåŠ 0.9åˆ†
        
        return keyword_boost + concept_boost
    
    def _weighted_fusion(self, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """ç®€å•åŠ æƒèåˆ"""
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x.similarity, reverse=True)
        
        for i, result in enumerate(results):
            result.rank = i + 1
            result.confidence = result.similarity
            if result.retrieval_source == 'faiss':
                result.confidence *= self.config.faiss_weight
            elif result.retrieval_source == 'vertex':
                result.confidence *= self.config.vertex_weight
        
        return results
    
    def _convert_to_retrieval_results(self, 
                                    results: List[Dict], 
                                    source: str) -> List[RetrievalResult]:
        """è½¬æ¢ä¸ºæ ‡å‡†çš„æ£€ç´¢ç»“æœæ ¼å¼"""
        retrieval_results = []
        
        for i, result in enumerate(results):
            # ä»ä¸åŒç³»ç»Ÿé€‚é…å­—æ®µ
            text = result.get('text', result.get('content_preview', ''))
            similarity = result.get('similarity', 1.0 - result.get('distance', 0.0))
            
            retrieval_result = RetrievalResult(
                id=result.get('id', result.get('datapoint_id', f'{source}_{i}')),
                text=text,
                source=result.get('source', 'unknown'),
                similarity=similarity,
                distance=result.get('distance', 1.0 - similarity),
                rank=result.get('rank', i + 1),
                retrieval_source=source,
                confidence=similarity,
                metadata=result.get('metadata', {})
            )
            retrieval_results.append(retrieval_result)
        
        return retrieval_results
    
    def update_config(self, **kwargs):
        """åŠ¨æ€æ›´æ–°é…ç½®"""
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
                print(f"ğŸ”§ é…ç½®æ›´æ–°: {key} = {value}")
    
    def get_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if self.stats['total_queries'] > 0:
            success_rate = (
                (self.stats['fast_success'] + self.stats['vertex_success'] + self.stats['hybrid_success']) 
                / self.stats['total_queries']
            )
        else:
            success_rate = 0.0
        
        return {
            **self.stats,
            'success_rate': success_rate,
            'fast_retrieval_available': self.fast_retrieval is not None,
            'config': {
                'faiss_weight': self.config.faiss_weight,
                'vertex_weight': self.config.vertex_weight,
                'strategy': 'hybrid_parallel'
            }
        }
    
    def health_check(self) -> Dict[str, bool]:
        """å¥åº·æ£€æŸ¥"""
        health = {
            'faiss_available': False,
            'vertex_available': False,
            'hybrid_available': False
        }
        
        # æ£€æŸ¥FAISS
        if self.fast_retrieval:
            try:
                stats = self.fast_retrieval.get_stats()
                health['faiss_available'] = stats['total_documents'] > 0
            except:
                health['faiss_available'] = False
        
        # æ£€æŸ¥Vertex AI (ç®€å•æµ‹è¯•)
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ ç®€å•çš„è¿é€šæ€§æµ‹è¯•
            health['vertex_available'] = bool(self.project_id and self.location)
        except:
            health['vertex_available'] = False
        
        health['hybrid_available'] = health['faiss_available'] or health['vertex_available']
        
        return health

# ä¾¿æ·å‡½æ•°ï¼šæ··åˆæ£€ç´¢æ¥å£
def hybrid_search(query: str, 
                 hybrid_retrieval: HybridRetrieval,
                 strategy: RetrievalStrategy = RetrievalStrategy.HYBRID_PARALLEL,
                 **kwargs) -> List[Dict]:
    """
    æ··åˆæ£€ç´¢ä¾¿æ·æ¥å£ - å…¼å®¹åŸæœ‰ç³»ç»Ÿ
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        hybrid_retrieval: æ··åˆæ£€ç´¢å®ä¾‹
        strategy: æ£€ç´¢ç­–ç•¥
        
    Returns:
        å…¼å®¹åŸæ ¼å¼çš„æ£€ç´¢ç»“æœ
    """
    results = hybrid_retrieval.search(query, strategy, **kwargs)
    
    # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
    compatible_results = []
    for result in results:
        compatible_result = {
            'id': result.id,
            'datapoint_id': result.id,
            'text': result.text,
            'source': result.source,
            'similarity': result.similarity,
            'distance': result.distance,
            'rank': result.rank,
            'retrieval_source': result.retrieval_source,
            'confidence': result.confidence,
            'content_preview': result.text[:100] + '...' if len(result.text) > 100 else result.text
        }
        compatible_results.append(compatible_result)
    
    return compatible_results 